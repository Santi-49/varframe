{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-varframe","title":"Welcome to VarFrame","text":"<p>Declarative DataFrame variable management with automatic DAG dependency resolution.</p> <p> </p> <p>VarFrame is a Python library designed to bring structure, reproducibility, and maintainability to your data science and machine learning pipelines. It transforms your data processing from a linear script into a robust, self-documenting graph of dependencies.</p>"},{"location":"#why-varframe","title":"Why VarFrame?","text":"<p>Data pipelines often start as simple scripts but quickly grow into unmanageable \"spaghetti code.\"</p> <ul> <li> <p>The Problem:</p> <ul> <li>Implicit Dependencies: \"Why did column <code>X</code> change? Oh, because I modified column <code>Y</code> 200 lines above.\"</li> <li>Execution Order Fragility: \"You have to run cell 4 before cell 2, but only if you skipped cell 3.\"</li> <li>Copy-Paste Logic: Feature engineering logic gets duplicated across training and inference pipelines, leading to drift.</li> </ul> </li> <li> <p>The VarFrame Solution:</p> <ul> <li>Explicit Dependencies: Every variable declares exactly what it needs to be computed.</li> <li>Automatic Resolution: VarFrame builds a Directed Acyclic Graph (DAG) and executes transformations in the mathematically correct order.</li> <li>Single Source of Truth: Your variable classes are your pipeline. Use the exact same classes for training, evaluation, and production inference.</li> </ul> </li> </ul>"},{"location":"#traditional-vs-varframe","title":"Traditional vs. VarFrame","text":"Feature Traditional (Pandas Script) VarFrame Logic Definition Imperative (<code>df['new'] = df['old'] * 2</code>) Declarative (Class <code>New</code> depends on <code>Old</code>) Execution Order Manually managed (top-to-bottop) Automatic (DAG-based topological sort) Reusability Low (Copy-paste code blocks) High (Import classes anywhere) Lazy Loading Manual caching or re-computation Built-in (Compute only when accessed) Documentation Comments scattered in code Self-documenting class structure"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Declarative Syntax: Define variables as Python classes.</li> <li>Automatic Dependency Resolution: Never worry about execution order again.</li> <li>Lazy Loading: Defer expensive computations until the data is actually needed.</li> <li>ML Integration: Built-in <code>BaseModel</code> and <code>ModelVariable</code> to treat model predictions just like any other column.</li> <li>Metadata Preserved: Export to Parquet with variable metadata intact.</li> <li>Data Integrity: Built-in hashing protects against schema drift and silent logic changes.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>VarFrame is available on PyPI:</p> <pre><code>pip install varframe\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Here is a simple example showing how VarFrame automatically resolves dependencies.</p> <pre><code>import pandas as pd\nfrom varframe import VarFrame, BaseVariable, DerivedVariable\n\n# 1. Define your variables as classes\nclass Radius(BaseVariable):\n    raw_column = \"radius\" # Maps to input DataFrame column\n    dtype = \"float\"\n\nclass Area(DerivedVariable):\n    # Dependencies are specific pointers to other variable classes\n    dependencies = [Radius]\n\n    @classmethod\n    def calculate(cls, df):\n        import math\n        # Access column by variable name\n        return math.pi * (df[Radius.name] ** 2)\n\n# 2. Initialize with data\ndata = pd.DataFrame({\"radius\": [1, 2, 5]})\nvf = VarFrame(data)\n\n# 3. Resolve dependencies\n# You ask for 'Area', VarFrame knows it needs 'Radius'\nvf.resolve(Area)\n\nprint(vf.df)\n#    radius       area\n# 0     1.0   3.141593\n# 1     2.0  12.566371\n# 2     5.0  78.539816\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the User Guide for deep dives into Lazy Loading, Machine Learning integration, and Data I/O.</li> <li>See the API Reference for detailed class documentation.</li> </ul>"},{"location":"api_reference/","title":"API Reference","text":"<p>Note: This page is auto-generated using <code>mkdocstrings</code>. If you are viewing this on GitHub, please visit the Online Documentation to see the full API reference.</p>"},{"location":"api_reference/#core","title":"Core","text":""},{"location":"api_reference/#varframe.VarFrame","title":"<code>varframe.VarFrame</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>A pandas DataFrame subclass with variable metadata and helper methods.</p> <p>VarFrame behaves exactly like a pandas DataFrame, but also maintains a registry of variable class definitions. This enables: - Filtering columns by variable type (BaseVariable vs DerivedVariable) - Accessing variable metadata and descriptions - Indexing by variable class (e.g., <code>vf[Gap]</code>)</p> <p>Attributes:</p> Name Type Description <code>_variables</code> <code>List[Type]</code> <p>List of variable classes (stored in _metadata).</p> Example <p>vf = VarFrame(df_raw, [Lap, Gap, GapDelta]) vf.head()              # Normal DataFrame operations work vf.filter_by_type(DerivedVariable)  # Variable-aware filtering vf[Gap]                # Access by variable class</p> Source code in <code>varframe/dataframe.py</code> <pre><code>class VarFrame(pd.DataFrame):\n    \"\"\"\n    A pandas DataFrame subclass with variable metadata and helper methods.\n\n    VarFrame behaves exactly like a pandas DataFrame, but also\n    maintains a registry of variable class definitions. This enables:\n    - Filtering columns by variable type (BaseVariable vs DerivedVariable)\n    - Accessing variable metadata and descriptions\n    - Indexing by variable class (e.g., ``vf[Gap]``)\n\n    Attributes:\n        _variables (List[Type]): List of variable classes (stored in _metadata).\n\n    Example:\n        &gt;&gt;&gt; vf = VarFrame(df_raw, [Lap, Gap, GapDelta])\n        &gt;&gt;&gt; vf.head()              # Normal DataFrame operations work\n        &gt;&gt;&gt; vf.filter_by_type(DerivedVariable)  # Variable-aware filtering\n        &gt;&gt;&gt; vf[Gap]                # Access by variable class\n    \"\"\"\n\n    # Tell pandas to preserve these attributes during operations\n    _metadata = [\"_variables\", \"_df_raw\", \"name\"]\n\n    def __init__(\n        self,\n        df_raw: Optional[pd.DataFrame] = None,\n        variables: Optional[VariableList] = None,\n        compute: bool = True,\n        auto_resolve: bool = True,\n        name: str = \"varframe\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize a VarFrame from raw data.\n\n        Args:\n            df_raw: The raw DataFrame to process. Stored internally for\n                future resolve() calls. If None, creates an empty DataFrame.\n            variables: List of variable classes to compute. If None or empty,\n                no variables are computed initially (use resolve() later).\n            compute: If True (default), compute the specified variables immediately.\n                If False, just store df_raw without computing anything.\n            auto_resolve: If True (default) and compute=True, automatically\n                resolve all dependencies. If False, variables must be in\n                correct dependency order.\n            **kwargs: Additional arguments passed to pd.DataFrame.__init__.\n\n        Example:\n            &gt;&gt;&gt; # Compute specific variables\n            &gt;&gt;&gt; vf = VarFrame(df_raw, [Lap, Gap, GapDelta])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Auto-resolve: just specify final variables\n            &gt;&gt;&gt; vf = VarFrame(df_raw, [PredictedGapDelta])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Store df_raw, compute nothing (use resolve() later)\n            &gt;&gt;&gt; vf = VarFrame(df_raw, compute=False)\n            &gt;&gt;&gt; vf.resolve(PredictedGapDelta)\n        \"\"\"\n        # Store raw DataFrame\n        self._df_raw: Optional[pd.DataFrame] = df_raw\n        self._variables: VariableList = []\n\n        # Determine which variables to compute\n        vars_to_compute: VariableList = []\n        if variables and compute:\n            if auto_resolve:\n                vars_to_compute = resolve_dependencies(variables)\n            else:\n                vars_to_compute = list(variables)\n\n            # Filter out lazy variables from initial computation\n            vars_to_compute = [\n                v\n                for v in vars_to_compute\n                if not (issubclass(v, DerivedVariable) and v.lazy)\n            ]\n\n        # Initialize DataFrame with proper index\n        if df_raw is not None:\n            super().__init__(index=df_raw.index, **kwargs)\n        else:\n            super().__init__(**kwargs)\n\n        self.name = name\n\n        # Compute variables if requested\n        if vars_to_compute and df_raw is not None:\n            for var in vars_to_compute:\n                if issubclass(var, BaseVariable):\n                    self[var.name] = var.compute(df_raw)\n                else:\n                    self[var.name] = var.compute(self)\n\n                if var not in self._variables:\n                    self._variables.append(var)\n\n            # Register remaining variables (lazy ones or non-computed) that were passed in\n            if variables:\n                 for var in variables:\n                     if var not in self._variables:\n                         self._variables.append(var)\n\n        elif variables:\n            # Store variable list even if not computing\n            self._variables = list(variables) if variables else []\n            # Ensure no duplicates if variables were passed\n            self._variables = list(dict.fromkeys(self._variables))\n\n    @property\n    def _constructor(self) -&gt; Type[VarFrame]:\n        \"\"\"Return the constructor for DataFrame operations to preserve type.\"\"\"\n        return _VarFrameInternal\n\n    @property\n    def variables(self) -&gt; VariableList:\n        \"\"\"Get the list of variable classes.\"\"\"\n        return self._variables\n\n    @variables.setter\n    def variables(self, value: VariableList) -&gt; None:\n        \"\"\"Set the list of variable classes.\"\"\"\n        self._variables = value\n\n    @property\n    def df_raw(self) -&gt; Optional[pd.DataFrame]:\n        \"\"\"Get the raw DataFrame (if stored).\"\"\"\n        return self._df_raw\n\n    @df_raw.setter\n    def df_raw(self, value: Optional[pd.DataFrame]) -&gt; None:\n        \"\"\"Set the raw DataFrame.\"\"\"\n        self._df_raw = value\n\n    def filter_by_type(\n        self, variable_type: Type[Union[BaseVariable, DerivedVariable]]\n    ) -&gt; VarFrame:\n        \"\"\"\n        Filter to include only columns of a specific variable type.\n\n        Args:\n            variable_type: The base class to filter by (BaseVariable or DerivedVariable).\n\n        Returns:\n            A new VarFrame containing only columns whose variables\n            are subclasses of the specified type.\n\n        Example:\n            &gt;&gt;&gt; derived_df = vf.filter_by_type(DerivedVariable)\n            &gt;&gt;&gt; base_df = vf.filter_by_type(BaseVariable)\n        \"\"\"\n        filtered_vars = [v for v in self._variables if issubclass(v, variable_type)]\n        filtered_names = [v.name for v in filtered_vars]\n        result = self[filtered_names].copy()\n        result._variables = filtered_vars\n        return result\n\n    def get_variable(self, name: str) -&gt; Optional[VariableType]:\n        \"\"\"\n        Retrieve a variable class by its name.\n\n        Args:\n            name: The name of the variable to retrieve.\n\n        Returns:\n            The variable class if found, None otherwise.\n        \"\"\"\n        for var in self._variables:\n            if var.name == name:\n                return var\n        return None\n\n    def list_variables(self) -&gt; List[str]:\n        \"\"\"\n        Get a list of all variable names.\n\n        Returns:\n            List of variable names in order.\n        \"\"\"\n        return [v.name for v in self._variables]\n\n    def describe_variables(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate a summary DataFrame describing all variables.\n\n        Returns:\n            A DataFrame with columns: name, type, dtype, description, etc.\n        \"\"\"\n        data = [var.info() for var in self._variables]\n        return pd.DataFrame(data)\n\n    def view(\n        self,\n        include: Optional[List[str]] = None,\n        variables: Optional[VariableList] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Create a DataFrame view containing only specific variables.\n\n        Args:\n            include: List of variable categories to include.\n                Options: 'base', 'derived', 'lazy', 'model', 'all'.\n                Defaults to ['all'] if both include and variables are None.\n            variables: Explicit list of variable classes to include.\n\n        Returns:\n            A pandas DataFrame with the requested variables. \n            Lazy variables will be computed on-demand for this view.\n\n        Example:\n            &gt;&gt;&gt; vf.view(include=['base', 'lazy'])\n            &gt;&gt;&gt; vf.view(variables=[LazySum])\n        \"\"\"\n        target_vars = []\n\n        # 1. Handle 'variables' argument\n        if variables:\n            target_vars.extend(variables)\n\n        # 2. Handle 'include' argument\n        if include:\n            for category in include:\n                if category == \"all\":\n                    target_vars.extend(self._variables)\n                elif category == \"base\":\n                    target_vars.extend(\n                        [v for v in self._variables if issubclass(v, BaseVariable)]\n                    )\n                elif category == \"derived\":\n                    # Non-lazy derived\n                    target_vars.extend(\n                        [\n                            v\n                            for v in self._variables\n                            if issubclass(v, DerivedVariable) and not v.lazy\n                        ]\n                    )\n                elif category == \"lazy\":\n                    # Lazy derived\n                    target_vars.extend(\n                        [\n                            v\n                            for v in self._variables\n                            if issubclass(v, DerivedVariable) and v.lazy\n                        ]\n                    )\n                elif category == \"model\":\n                    target_vars.extend(\n                        [\n                            v\n                            for v in self._variables\n                            if hasattr(v, \"model_class\") and v.model_class\n                        ]\n                    )\n\n        # Default to 'all' if nothing specified\n        if not target_vars:\n            target_vars = list(self._variables)\n\n        # Remove duplicates while preserving order\n        target_vars = list(dict.fromkeys(target_vars))\n\n        # 3. Construct DataFrame\n        data = {}\n        for var in target_vars:\n            # If standard column, use it (zero copy if possible)\n            if var.name in self.columns:\n                data[var.name] = self[var.name]\n            else:\n                # Must be lazy or missing -&gt; Compute it\n                data[var.name] = var.compute(self)\n\n        return pd.DataFrame(data, index=self.index)\n\n    def __getitem__(self, key: Union[str, VariableType, List, slice, pd.Series]) -&gt; Any:\n        \"\"\"\n        Access columns by name, variable class, or standard DataFrame indexing.\n\n        Extends pandas indexing to support variable classes directly.\n\n        Args:\n            key: Column name (str), variable class, list of columns,\n                 boolean Series, or slice.\n\n        Returns:\n            Series for single column, DataFrame for multiple columns.\n\n        Example:\n            &gt;&gt;&gt; vf[\"gap\"]      # By string name\n            &gt;&gt;&gt; vf[Gap]        # By variable class\n            &gt;&gt;&gt; vf[[Lap, Gap]] # Multiple variable classes\n        \"\"\"\n        # Handle variable class\n        if isinstance(key, type) and issubclass(key, (BaseVariable, DerivedVariable)):\n            # Check if it's lazy and missing (Compute just-in-time)\n            if (\n                issubclass(key, DerivedVariable)\n                and key.lazy\n                and key.name not in self.columns\n            ):\n                 return key.compute(self)\n\n            return super().__getitem__(key.name)\n\n        # Handle list of variable classes\n        if isinstance(key, list) and len(key) &gt; 0:\n            if isinstance(key[0], type) and issubclass(\n                key[0], (BaseVariable, DerivedVariable)\n            ):\n                names = [k.name for k in key]\n                return super().__getitem__(names)\n\n        # Default pandas behavior\n        try:\n            return super().__getitem__(key)\n        except KeyError:\n            # Check for lazy variable (string access)\n            if isinstance(key, str):\n                var = self.get_variable(key)\n\n                # Case 1: Variable is registered but not in columns (Lazy or missing)\n                if var and issubclass(var, DerivedVariable) and var.lazy:\n                    return var.compute(self)\n\n            # Re-raise if not handled\n            raise\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation.\"\"\"\n        var_info = f\"Variables: {len(self._variables)} ({self.list_variables()})\"\n        df_repr = super().__repr__()\n        return f\"{var_info}\\n{df_repr}\"\n\n    # ------------------- ML Compatibility Methods -------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert to a plain pandas DataFrame.\n\n        Use this before passing to ML libraries that may have issues\n        with DataFrame subclasses (e.g., pickle, joblib, some sklearn pipelines).\n\n        Returns:\n            A plain pandas DataFrame with the same data (no variable metadata).\n\n        Example:\n            &gt;&gt;&gt; plain_df = vf.to_pandas()\n            &gt;&gt;&gt; model.fit(plain_df, y)\n        \"\"\"\n        return pd.DataFrame(self)\n\n    def to_ml(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Alias for to_pandas(). Explicit conversion for ML pipelines.\n\n        Use this to clearly indicate the DataFrame is being prepared\n        for machine learning operations.\n\n        Returns:\n            A plain pandas DataFrame suitable for ML libraries.\n\n        Example:\n            &gt;&gt;&gt; X_train = vf.to_ml()\n            &gt;&gt;&gt; model.fit(X_train, y_train)\n        \"\"\"\n        return pd.DataFrame(self)\n\n    def to_csv(\n        self,\n        path_or_buf: Optional[Union[str, Any]] = None,\n        *args: Any,\n        include: Optional[List[str]] = None,\n        variables: Optional[VariableList] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[str]:\n        \"\"\"\n        Write object to a comma-separated values (csv) file.\n\n        Enhancements over pandas.to_csv:\n        - Supports `include` and `variables` to compute lazy variables on-the-fly.\n        - Warns if registered variables are not included in the export.\n        - Defaults to {self.name}.csv if path is not provided.\n        \"\"\"\n        from varframe.config import VFConfig\n\n        # 1. Resolve Data\n        if include or variables:\n            df_to_export = self.view(include=include, variables=variables)\n        else:\n            df_to_export = self\n\n        # 2. Prevent implicit data loss (Warn if variables are missing)\n        # Check which variables are in the export\n        exported_cols = set(df_to_export.columns)\n        missing_vars = [\n            v.name for v in self._variables \n            if v.name not in exported_cols and v.name not in self.columns\n        ]\n\n        # Also check for variables present in self but not in export (if view filtered them)\n        excluded_present_vars = [\n             v.name for v in self._variables\n             if v.name in self.columns and v.name not in exported_cols\n        ]\n\n        if missing_vars:\n             path_str = str(path_or_buf) if path_or_buf else \"output\"\n             VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_COMPUTE, # Reusing generic warning op\n                f\"Exporting to {path_str} without computing lazy variables: {missing_vars}. \"\n                \"Use `include=['all']` or `variables=[...]` to compute them during export.\",\n             )\n\n        # 3. Resolve Path\n        if path_or_buf is None:\n            path_or_buf = f\"{self.name}.csv\"\n\n        if hasattr(df_to_export, \"to_pandas\"):\n            return df_to_export.to_pandas().to_csv(path_or_buf, *args, **kwargs)\n        else:\n            return df_to_export.to_csv(path_or_buf, *args, **kwargs)\n\n    def to_parquet(\n        self,\n        path: Optional[Union[str, Any]] = None,\n        *args: Any,\n        include: Optional[List[str]] = None,\n        variables: Optional[VariableList] = None,\n        **kwargs: Any,\n    ) -&gt; Optional[bytes]:\n        \"\"\"\n        Write object to a binary parquet file.\n\n        Enhancements over pandas.to_parquet:\n        - Supports `include` and `variables` to compute lazy variables on-the-fly.\n        - Warns if registered variables are not included in the export.\n        - Defaults to {self.name}.parquet if path is not provided.\n        - Saves variable names in file metadata.\n        \"\"\"\n        import json\n        from varframe.config import VFConfig\n\n        # 1. Resolve Data\n        if include or variables:\n            df_to_export = self.view(include=include, variables=variables)\n        else:\n            df_to_export = self\n\n        # 2. Prevent implicit data loss\n        exported_cols = set(df_to_export.columns)\n        missing_vars = [\n            v.name for v in self._variables \n            if v.name not in exported_cols and v.name not in self.columns\n        ]\n\n        if missing_vars:\n             path_str = str(path) if path else \"output\"\n             VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                f\"Exporting to {path_str} without computing lazy variables: {missing_vars}. \"\n                \"Use `include=['all']` or `variables=[...]` to compute them during export.\",\n             )\n\n        # 3. Resolve Path\n        if path is None:\n            path = f\"{self.name}.parquet\"\n\n        # 4. Prepare Metadata (Strategy B)\n        # Get existing keyword args or creating new dict\n        # pyarrow.Table.from_pandas uses 'preserve_index' etc.\n        # to_parquet kwargs are passed to the engine. \n        # For pyarrow engine (default), we can't easily inject metadata via to_parquet directly \n        # because pandas abstracts this. \n        # WORKAROUND: We will use table properties if using pyarrow, \n        # but to keep it simple and pandas-compliant, we might need a distinct approach.\n        # Actually, pandas &gt;= 1.0 does not easily support custom metadata in to_parquet \n        # without dropping down to pyarrow directly.\n\n        try:\n            import pyarrow as pa\n            import pyarrow.parquet as pq\n\n            # Convert to Table\n            # Handle VarFrame or plain DataFrame\n            if hasattr(df_to_export, \"to_pandas\"):\n                df_plain = df_to_export.to_pandas()\n            else:\n                df_plain = df_to_export\n\n            table = pa.Table.from_pandas(df_plain)\n\n            # Add Metadata\n            custom_meta = {\n                \"varframe_variables\": json.dumps([v.name for v in self._variables])\n            }\n\n            # Add Hash Metadata\n            try:\n                hashes = {v.name: v.get_hash_components() for v in self._variables}\n                custom_meta[\"varframe_hashes\"] = json.dumps(hashes)\n            except Exception as e:\n                VFConfig.warn(\n                    ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                    f\"Failed to compute variable hashes for export: {e}\"\n                )\n            # Combine with existing metadata\n            existing_meta = table.schema.metadata or {}\n            combined_meta = {**existing_meta, **{k.encode(): v.encode() for k, v in custom_meta.items()}}\n            table = table.replace_schema_metadata(combined_meta)\n\n            # Write\n            pq.write_table(table, path, *args, **kwargs)\n            return None\n\n        except ImportError:\n            # Fallback for when pyarrow is not available or user wants different engine\n             VFConfig.warn(\n                 ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                 \"Pyarrow not found or failed. Exporting without VarFrame metadata.\"\n             )\n             if hasattr(df_to_export, \"to_pandas\"):\n                 return df_to_export.to_pandas().to_parquet(path, *args, **kwargs)\n             else:\n                 return df_to_export.to_parquet(path, *args, **kwargs)\n\n    def add_variables(\n        self,\n        *variables: VariableType,\n        compute: bool = True,\n        suppress_warnings: bool = False,\n    ) -&gt; VarFrame:\n        \"\"\"\n        Register and optionally compute new variables (in-place).\n\n        Args:\n            *variables: Variable classes to add.\n            compute: If True (default), compute variables immediately.\n                If False, just register them (must already exist in DataFrame).\n            suppress_warnings: If True, suppress warnings for this call only.\n\n        Returns:\n            Self, for method chaining.\n\n        Raises:\n            KeyError: If compute=True and dependencies are missing.\n            RuntimeError: If implicit usage is disabled in VFConfig.\n            ValueError: If compute=True and adding a BaseVariable (must come from raw).\n        \"\"\"\n        for var in variables:\n            if compute:\n                # --- Compute Mode ---\n                if var.name in self.columns:\n                    continue  # Already exists\n\n                # Note: Explicit addition via add_variables does not trigger implicit warnings.\n\n                if issubclass(var, BaseVariable):\n                    raise ValueError(\n                        f\"Cannot add BaseVariable '{var.name}' to existing DataFrame. \"\n                        \"BaseVariables can only be computed from raw data.\"\n                    )\n                else:\n                    self[var.name] = var.compute(self)\n\n            else:\n                # --- No Compute Mode (Registration Only) ---\n                # Explicit registration - no warning needed.\n                pass\n\n            # Common: Add to registry\n            if var not in self._variables:\n                self._variables.append(var)\n\n        return self\n\n    def add_variable(\n        self,\n        *variables: VariableType,\n        compute: bool = True,\n        suppress_warnings: bool = False,\n    ) -&gt; VarFrame:\n        \"\"\"\n        Alias for add_variables.\n        \"\"\"\n        return self.add_variables(\n            *variables, compute=compute, suppress_warnings=suppress_warnings\n        )\n\n    def resolve(\n        self,\n        *target_variables: VariableType,\n        suppress_warnings: bool = False,\n    ) -&gt; VarFrame:\n        \"\"\"\n        Resolve and compute target variables with automatic dependency resolution.\n\n        This method automatically determines all missing dependencies for the\n        target variables, computes them in the correct DAG order, and adds\n        them to the DataFrame.\n\n        Args:\n            *target_variables: Variable classes to resolve and compute.\n            suppress_warnings: If True, suppress warnings for this call only.\n\n        Returns:\n            Self, for method chaining.\n\n        Raises:\n            ValueError: If circular dependencies are detected.\n            ValueError: If a BaseVariable is needed but df_raw is not available.\n            RuntimeError: If implicit operations are disabled in VFConfig.\n\n        Example:\n            &gt;&gt;&gt; vf = VarFrame(df_raw, [Lap, Gap])\n            &gt;&gt;&gt; vf.resolve(PredictedGapDelta)\n        \"\"\"\n        # Resolve all dependencies\n        all_vars = resolve_dependencies(list(target_variables))\n\n        # Filter to only missing variables\n        # Filter to only missing variables\n        missing_vars = [v for v in all_vars if v.name not in self.columns]\n\n        # Split into compute-now vs lazy\n        lazy_vars = [\n            v for v in missing_vars if issubclass(v, DerivedVariable) and v.lazy\n        ]\n        missing_vars = [v for v in missing_vars if v not in lazy_vars]\n\n        # Check for missing BaseVariables - need df_raw to compute them\n        missing_base = [v for v in missing_vars if issubclass(v, BaseVariable)]\n        if missing_base and self._df_raw is None:\n            names = [v.name for v in missing_base]\n            raise ValueError(\n                f\"Cannot resolve BaseVariables {names}: no raw DataFrame stored. \"\n                \"Pass df_raw to VarFrame() or set vf.df_raw to enable.\"\n            )\n\n        # Warn about implicit computation of missing variables\n        if missing_vars and not suppress_warnings:\n            var_names = [v.name for v in missing_vars]\n            VFConfig.check_permission(\n                ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                f\"Cannot resolve variables {var_names}. \"\n                \"Set VFConfig.allow_implicit_compute = True to enable.\",\n            )\n            VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                f\"Implicitly computing {len(missing_vars)} variable(s) not in _variables: {var_names}\",\n                stacklevel=3,\n            )\n\n        # Register lazy variables (so get_variable works)\n        for var in lazy_vars:\n            if var not in self._variables:\n                self._variables.append(var)\n\n        if suppress_warnings:\n            context = VFConfig.suppress_warnings()\n        else:\n            context = VFConfig.null_context()\n\n        with context:\n            # Compute missing variables in order\n            for var in missing_vars:\n                if issubclass(var, BaseVariable):\n                    self[var.name] = var.compute(self._df_raw)\n                else:\n                    self[var.name] = var.compute(self)\n\n                if var not in self._variables:\n                    self._variables.append(var)\n\n        return self\n\n    def explain_calculation(\n        self,\n        target_variables: VariableList,\n        legend: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Explain the calculation plan for the given variables given the current DataFrame state.\n\n        Prints a color-coded list indicating:\n        - Variable Type (Base, Derived, Model)\n        - Calculation Status (Ready vs Needs Calculation)\n        - Warnings (Implicit computation, Auto-training, Inference)\n\n        Args:\n            target_variables: List of variable classes to explain.\n            legend: If True, print a legend for the warning codes.\n        \"\"\"\n        from varframe.variables import BaseVariable\n        from varframe.config import VFConfig, ImplicitOperation\n\n        # ANSI color codes\n        GREEN = \"\\033[92m\"\n        YELLOW = \"\\033[93m\"\n        PURPLE = \"\\033[95m\"\n        ORANGE = \"\\033[33m\"\n        GREY = \"\\033[90m\"\n        RESET = \"\\033[0m\"\n\n        resolved = resolve_dependencies(target_variables)\n\n        names = \", \".join(v.name for v in target_variables)\n        print(f\"Calculation Plan for {names}:\")\n\n        used_codes = set()\n        # Mapping: Code -&gt; (Description, ConfigCheck)\n        warning_defs = {\n            \"I\": (\"Implicit Compute\", VFConfig.warn_add_variable_compute),\n            \"T\": (\"Auto-Train Model\", VFConfig.warn_train_model),\n            \"M\": (\"Model Inference\", VFConfig.warn_infer_model),\n        }\n\n        for i, v in enumerate(resolved, 1):\n            # 1. Determine Type\n            if hasattr(v, \"model_class\") and v.model_class:\n                var_type = \"model_prediction\"\n                color = PURPLE\n            elif issubclass(v, BaseVariable):\n                var_type = \"base\"\n                color = GREEN\n            else:\n                var_type = \"derived\"\n                color = YELLOW\n\n            # 2. Determine Status\n            exists = v.name in self.columns\n            status_icon = \"\u2705\" if exists else \"\u23f3\"\n\n            # 3. Predict Warnings\n            current_codes = []\n            if not exists:\n                # Implicit Variable Warning\n                if v not in self._variables and warning_defs[\"I\"][1]:\n                     current_codes.append(\"I\")\n\n                # Model Warnings\n                if var_type == \"model_prediction\" and v.model_class:\n                    if not v.model_class.is_trained and warning_defs[\"T\"][1]:\n                         current_codes.append(\"T\")\n                    if warning_defs[\"M\"][1]:\n                        current_codes.append(\"M\")\n\n            used_codes.update(current_codes)\n\n            # Formatting\n            line = f\"  {i}. {status_icon} {color}{v.name}{RESET} ({var_type})\"\n            if current_codes:\n                codes_str = \",\".join(current_codes)\n                # Subtle grey for the warning hints\n                line += f\" {GREY}\u26a0 [{codes_str}]{RESET}\"\n\n            print(line)\n\n        if legend and used_codes:\n            print(\"\\nLegend:\")\n            for code in sorted(used_codes):\n                desc = warning_defs[code][0]\n                print(f\"  {GREY}\u26a0 [{code}]{RESET} : {desc}\")\n        print()\n\n    @classmethod\n    def from_pandas(\n        cls,\n        df: pd.DataFrame,\n        variables: Optional[VariableList] = None,\n        df_raw: Optional[pd.DataFrame] = None,\n        name: str = \"varframe\",\n    ) -&gt; VarFrame:\n        \"\"\"\n        Create a VarFrame from a plain pandas DataFrame.\n\n        Use this to re-wrap a DataFrame after ML operations or when\n        loading data that was previously converted with to_pandas().\n\n        Args:\n            df: The pandas DataFrame to convert (already processed data).\n            variables: List of variable classes to associate with the DataFrame.\n            df_raw: Optional raw DataFrame to store for future resolve() calls.\n\n        Returns:\n            A new VarFrame with the given data and variables.\n\n        Example:\n            &gt;&gt;&gt; plain_df = pd.read_pickle(\"data.pkl\")\n            &gt;&gt;&gt; vf = VarFrame.from_pandas(plain_df, variables=[Lap, Gap])\n        \"\"\"\n        vf = cls(df_raw=None, variables=None, compute=False, name=name)\n        for col in df.columns:\n            vf[col] = df[col].values\n        vf.index = df.index\n        vf._variables = list(variables) if variables else []\n        vf._df_raw = df_raw\n        return vf\n\n    @staticmethod\n    def _discover_all_variables() -&gt; Dict[str, List[VariableType]]:\n        \"\"\"\n        Recursively discover all BaseVariable and DerivedVariable subclasses.\n\n        Returns:\n            Dict mapping variable name to a list of class definitions (to handle redefinitions).\n        \"\"\"\n        discovered = {}\n\n        def _scan(cls):\n            # Add self if it's a concrete variable (has a name)\n            if hasattr(cls, \"name\") and cls.name:\n                if cls.name not in discovered:\n                    discovered[cls.name] = []\n                # Avoid duplicates in the list\n                if cls not in discovered[cls.name]:\n                     discovered[cls.name].append(cls)\n\n            # Recurse\n            for sub in cls.__subclasses__():\n                _scan(sub)\n\n        _scan(BaseVariable)\n        _scan(DerivedVariable)\n        return discovered\n\n    @classmethod\n    def load_csv(\n        cls, \n        path_or_buf: Union[str, Any], \n        variables: Optional[List[VariableType]] = None,\n        exclude: Optional[List[VariableType]] = None,\n        discard_unmatched: bool = True,\n        ambiguity: Optional[Dict[str, VariableType]] = None,\n        **kwargs: Any\n    ) -&gt; VarFrame:\n        \"\"\"\n        Load a VarFrame from a CSV file, automatically discovering variables.\n\n        This method scans the current Python environment for variable definitions\n        that match the columns in the CSV.\n\n        Args:\n            path_or_buf: Path to the CSV file.\n            variables: Whitelist of variable classes to load. If provided, only \n                these variables will be matched. Acts as implicit disambiguation.\n            exclude: Blacklist of variable classes to exclude from loading.\n                Cannot be used together with `variables`.\n            discard_unmatched: If True (default), columns not matching any known\n                variable are dropped. If False, they are kept as plain columns.\n            ambiguity: Dict mapping variable names to specific classes for \n                disambiguation when multiple definitions exist with the same name.\n            **kwargs: Arguments passed to pd.read_csv.\n\n        Returns:\n            A reconstructed VarFrame.\n\n        Raises:\n            ValueError: If both `variables` and `exclude` are provided.\n            AmbiguityError: If multiple variable definitions match a column name\n                and no disambiguation is provided.\n        \"\"\"\n        from varframe.exceptions import AmbiguityError\n\n        # Validation\n        if variables is not None and exclude is not None:\n            raise ValueError(\"Cannot use both 'variables' and 'exclude' parameters together.\")\n\n        df = pd.read_csv(path_or_buf, **kwargs)\n\n        # Auto-discovery\n        known_vars = cls._discover_all_variables()\n        matched_vars = []\n\n        # Build target names based on whitelist/blacklist\n        if variables is not None:\n            target_names = {v.name for v in variables}\n            # Create a lookup for whitelisted variables by name\n            whitelist_by_name: Dict[str, VariableType] = {v.name: v for v in variables}\n        elif exclude is not None:\n            exclude_names = {v.name for v in exclude}\n            target_names = {col for col in df.columns if col not in exclude_names}\n            whitelist_by_name = None\n        else:\n            target_names = set(df.columns)\n            whitelist_by_name = None\n\n        for col in df.columns:\n            if col not in target_names:\n                continue\n\n            if col not in known_vars:\n                continue\n\n            candidates = known_vars[col]\n\n            # If whitelist mode, use the whitelisted class directly\n            if whitelist_by_name is not None:\n                if col in whitelist_by_name:\n                    matched_vars.append(whitelist_by_name[col])\n                continue\n\n            # Check for ambiguity\n            if len(candidates) &gt; 1:\n                # Check if disambiguation provided via ambiguity parameter\n                if ambiguity and col in ambiguity:\n                    matched_vars.append(ambiguity[col])\n                else:\n                    raise AmbiguityError(\n                        f\"Ambiguous variable '{col}'. Multiple definitions found: \"\n                        f\"{[c.__name__ for c in candidates]}. \"\n                        f\"Use 'ambiguity' parameter to specify which to use, e.g.: \"\n                        f\"ambiguity={{'{col}': {candidates[0].__name__}}}\"\n                    )\n            else:\n                matched_vars.append(candidates[0])\n\n        # Handle unmatched columns\n        mapped_names = {v.name for v in matched_vars}\n        unmapped_cols = [col for col in df.columns if col not in mapped_names]\n\n        if discard_unmatched:\n            # Drop unmatched columns\n            cols_to_keep = [v.name for v in matched_vars]\n            df = df[[c for c in cols_to_keep if c in df.columns]]\n        else:\n            # Warn about unmapped columns (old behavior)\n            if unmapped_cols:\n                from varframe.config import VFConfig, ImplicitOperation\n                VFConfig.warn(\n                    ImplicitOperation.ADD_VARIABLE_NO_COMPUTE,\n                    f\"Loaded columns {unmapped_cols} do not match any known Variable class. \"\n                    \"They will be loaded as plain pandas columns.\"\n                )\n\n        # Try to infer name from path\n        name = \"varframe\"\n        if isinstance(path_or_buf, str):\n            import os\n            name = os.path.splitext(os.path.basename(path_or_buf))[0]\n\n        return cls.from_pandas(df, variables=matched_vars, name=name)\n\n    @classmethod\n    def load_parquet(\n        cls, \n        path: Union[str, Any], \n        variables: Optional[List[VariableType]] = None,\n        exclude: Optional[List[VariableType]] = None,\n        discard_unmatched: bool = True,\n        ambiguity: Optional[Dict[str, VariableType]] = None,\n        **kwargs: Any\n    ) -&gt; VarFrame:\n        \"\"\"\n        Load a VarFrame from a Parquet file, using metadata or auto-discovery.\n\n        1. Checks for 'varframe_variables' metadata in the file.\n        2. If found, looks up those variables in the environment.\n        3. If not found, falls back to matching column names (like load_csv).\n\n        Args:\n            path: Path to the Parquet file.\n            variables: Whitelist of variable classes to load. If provided, only \n                these variables will be matched. Acts as implicit disambiguation\n                and overrides hash-based resolution.\n            exclude: Blacklist of variable classes to exclude from loading.\n                Cannot be used together with `variables`.\n            discard_unmatched: If True (default), columns not matching any known\n                variable are dropped. If False, they are kept as plain columns.\n            ambiguity: Dict mapping variable names to specific classes for \n                disambiguation. Overrides hash-based resolution for specified names.\n            **kwargs: Arguments passed to pyarrow/pandas read functions.\n\n        Returns:\n            A reconstructed VarFrame.\n\n        Raises:\n            ValueError: If both `variables` and `exclude` are provided.\n        \"\"\"\n        import json\n\n        # Validation\n        if variables is not None and exclude is not None:\n            raise ValueError(\"Cannot use both 'variables' and 'exclude' parameters together.\")\n\n        # ANSI Colors\n        RED = \"\\033[91m\"\n        ORANGE = \"\\033[33m\"\n        WHITE = \"\\033[37m\"\n        RESET = \"\\033[0m\"\n\n        warnings_list = []\n\n        # Try reading metadata first (requires pyarrow)\n        file_hashes = {}\n        var_names_from_meta = []\n        all_columns = []\n\n        try:\n            import pyarrow.parquet as pq\n            meta = pq.read_metadata(path)\n            # Get column names from schema\n            all_columns = list(meta.schema.names)\n            if meta.metadata:\n                if b'varframe_variables' in meta.metadata:\n                    var_names_from_meta = json.loads(meta.metadata[b'varframe_variables'])\n                if b'varframe_hashes' in meta.metadata:\n                    file_hashes = json.loads(meta.metadata[b'varframe_hashes'])\n        except ImportError:\n            pass\n\n        known_vars = cls._discover_all_variables()\n\n        # Determine which columns to load based on parameters\n        if variables is not None:\n            # Whitelist mode - use provided classes directly\n            target_names = [v.name for v in variables]\n            whitelist_by_name: Dict[str, VariableType] = {v.name: v for v in variables}\n        elif exclude is not None:\n            exclude_names = {v.name for v in exclude}\n            source_names = var_names_from_meta if var_names_from_meta else all_columns\n            target_names = [n for n in source_names if n not in exclude_names]\n            whitelist_by_name = None\n        else:\n            target_names = None  # Load all\n            whitelist_by_name = None\n\n        # Use pyarrow column selection for efficiency\n        try:\n            import pyarrow.parquet as pq\n            if target_names is not None:\n                # Only read requested columns\n                cols_to_read = [c for c in target_names if c in all_columns]\n                table = pq.read_table(path, columns=cols_to_read, **kwargs)\n            else:\n                table = pq.read_table(path, **kwargs)\n            df = table.to_pandas()\n        except ImportError:\n            # Fallback to pandas (less efficient)\n            df = pd.read_parquet(path, **kwargs)\n            if target_names is not None:\n                available = [c for c in target_names if c in df.columns]\n                df = df[available]\n\n        matched_vars = []\n\n        # Determine variable names to look for\n        if target_names is not None:\n            names_to_resolve = target_names\n        else:\n            names_to_resolve = var_names_from_meta if var_names_from_meta else df.columns\n\n        for name in names_to_resolve:\n            # Handle case where name is in metadata but not in known_vars (not imported)\n            if name not in known_vars:\n                continue\n\n            # If whitelist mode, use the whitelisted class directly\n            if whitelist_by_name is not None:\n                if name in whitelist_by_name:\n                    matched_vars.append(whitelist_by_name[name])\n                continue\n\n            candidates = known_vars[name]\n\n            # Check for multiple candidates (ambiguity)\n            if len(candidates) &gt; 1:\n                # Require explicit disambiguation\n                if ambiguity and name in ambiguity:\n                    matched_vars.append(ambiguity[name])\n                else:\n                    from varframe.exceptions import AmbiguityError\n                    raise AmbiguityError(\n                        f\"Ambiguous variable '{name}'. Multiple definitions found: \"\n                        f\"{[c.__name__ for c in candidates]}. \"\n                        f\"Use 'ambiguity' parameter to specify which to use, e.g.: \"\n                        f\"ambiguity={{'{name}': {candidates[0].__name__}}}\"\n                    )\n            else:\n                # Single candidate - use it\n                matched_vars.append(candidates[0])\n\n        # Handle unmatched columns\n        mapped_names = {v.name for v in matched_vars}\n        unmapped_cols = [col for col in df.columns if col not in mapped_names]\n\n        if discard_unmatched:\n            # Drop unmatched columns\n            cols_to_keep = [v.name for v in matched_vars]\n            df = df[[c for c in cols_to_keep if c in df.columns]]\n        else:\n            # Warn about unmapped columns (old behavior)\n            if unmapped_cols:\n                from varframe.config import VFConfig, ImplicitOperation\n                VFConfig.warn(\n                    ImplicitOperation.ADD_VARIABLE_NO_COMPUTE,\n                    f\"Loaded columns {unmapped_cols} do not match any known Variable class. \"\n                    \"They will be loaded as plain pandas columns.\"\n                )\n\n        # Try to infer name from path\n        vf_name = \"varframe\"\n        if isinstance(path, str):\n            import os\n            vf_name = os.path.splitext(os.path.basename(path))[0]\n\n        # ------------------- Integrity Check -------------------\n        # Compare loaded variables against current environment\n\n        if file_hashes:\n            for var in matched_vars:\n                if var.name not in file_hashes:\n                    continue\n\n                stored = file_hashes[var.name]\n                current = var.get_hash_components()\n\n                # Check for changes\n                diffs = []\n                severity = 0 # 0=None, 1=White, 2=Orange, 3=Red\n\n                # 1. Calculation (RED)\n                if stored.get(\"calc\") != current[\"calc\"]:\n                    diffs.append(\"Calculation logic changed\")\n                    severity = max(severity, 3)\n\n                # 2. Dependencies (RED)\n                if stored.get(\"deps\") != current[\"deps\"]:\n                    diffs.append(\"Dependencies changed\")\n                    severity = max(severity, 3)\n\n                # 3. Attributes (ORANGE) - dtype\n                if stored.get(\"attrs\") != current[\"attrs\"]:\n                    diffs.append(\"Attributes (dtype) changed\")\n                    severity = max(severity, 2)\n\n                # 4. Metadata (WHITE) - description, lazy\n                if stored.get(\"meta\") != current[\"meta\"]:\n                    diffs.append(\"Metadata (desc/lazy) changed\")\n                    severity = max(severity, 1)\n\n                if diffs:\n                    color = WHITE\n                    if severity == 3: color = RED\n                    elif severity == 2: color = ORANGE\n\n                    msg = f\"{color}Variable '{var.name}': {', '.join(diffs)}{RESET}\"\n                    warnings_list.append(msg)\n\n        if warnings_list:\n            print(f\"\\n{ORANGE}VarFrame Integrity Check Warnings:{RESET}\")\n            for w in warnings_list:\n                print(f\"  {w}\")\n\n        return cls.from_pandas(df, variables=matched_vars, name=vf_name)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.df_raw","title":"<code>df_raw</code>  <code>property</code> <code>writable</code>","text":"<p>Get the raw DataFrame (if stored).</p>"},{"location":"api_reference/#varframe.VarFrame.variables","title":"<code>variables</code>  <code>property</code> <code>writable</code>","text":"<p>Get the list of variable classes.</p>"},{"location":"api_reference/#varframe.VarFrame.add_variable","title":"<code>add_variable(*variables, compute=True, suppress_warnings=False)</code>","text":"<p>Alias for add_variables.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def add_variable(\n    self,\n    *variables: VariableType,\n    compute: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; VarFrame:\n    \"\"\"\n    Alias for add_variables.\n    \"\"\"\n    return self.add_variables(\n        *variables, compute=compute, suppress_warnings=suppress_warnings\n    )\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.add_variables","title":"<code>add_variables(*variables, compute=True, suppress_warnings=False)</code>","text":"<p>Register and optionally compute new variables (in-place).</p> <p>Parameters:</p> Name Type Description Default <code>*variables</code> <code>VariableType</code> <p>Variable classes to add.</p> <code>()</code> <code>compute</code> <code>bool</code> <p>If True (default), compute variables immediately. If False, just register them (must already exist in DataFrame).</p> <code>True</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress warnings for this call only.</p> <code>False</code> <p>Returns:</p> Type Description <code>VarFrame</code> <p>Self, for method chaining.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If compute=True and dependencies are missing.</p> <code>RuntimeError</code> <p>If implicit usage is disabled in VFConfig.</p> <code>ValueError</code> <p>If compute=True and adding a BaseVariable (must come from raw).</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def add_variables(\n    self,\n    *variables: VariableType,\n    compute: bool = True,\n    suppress_warnings: bool = False,\n) -&gt; VarFrame:\n    \"\"\"\n    Register and optionally compute new variables (in-place).\n\n    Args:\n        *variables: Variable classes to add.\n        compute: If True (default), compute variables immediately.\n            If False, just register them (must already exist in DataFrame).\n        suppress_warnings: If True, suppress warnings for this call only.\n\n    Returns:\n        Self, for method chaining.\n\n    Raises:\n        KeyError: If compute=True and dependencies are missing.\n        RuntimeError: If implicit usage is disabled in VFConfig.\n        ValueError: If compute=True and adding a BaseVariable (must come from raw).\n    \"\"\"\n    for var in variables:\n        if compute:\n            # --- Compute Mode ---\n            if var.name in self.columns:\n                continue  # Already exists\n\n            # Note: Explicit addition via add_variables does not trigger implicit warnings.\n\n            if issubclass(var, BaseVariable):\n                raise ValueError(\n                    f\"Cannot add BaseVariable '{var.name}' to existing DataFrame. \"\n                    \"BaseVariables can only be computed from raw data.\"\n                )\n            else:\n                self[var.name] = var.compute(self)\n\n        else:\n            # --- No Compute Mode (Registration Only) ---\n            # Explicit registration - no warning needed.\n            pass\n\n        # Common: Add to registry\n        if var not in self._variables:\n            self._variables.append(var)\n\n    return self\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.describe_variables","title":"<code>describe_variables()</code>","text":"<p>Generate a summary DataFrame describing all variables.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns: name, type, dtype, description, etc.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def describe_variables(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a summary DataFrame describing all variables.\n\n    Returns:\n        A DataFrame with columns: name, type, dtype, description, etc.\n    \"\"\"\n    data = [var.info() for var in self._variables]\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.explain_calculation","title":"<code>explain_calculation(target_variables, legend=False)</code>","text":"<p>Explain the calculation plan for the given variables given the current DataFrame state.</p> <p>Prints a color-coded list indicating: - Variable Type (Base, Derived, Model) - Calculation Status (Ready vs Needs Calculation) - Warnings (Implicit computation, Auto-training, Inference)</p> <p>Parameters:</p> Name Type Description Default <code>target_variables</code> <code>VariableList</code> <p>List of variable classes to explain.</p> required <code>legend</code> <code>bool</code> <p>If True, print a legend for the warning codes.</p> <code>False</code> Source code in <code>varframe/dataframe.py</code> <pre><code>def explain_calculation(\n    self,\n    target_variables: VariableList,\n    legend: bool = False,\n) -&gt; None:\n    \"\"\"\n    Explain the calculation plan for the given variables given the current DataFrame state.\n\n    Prints a color-coded list indicating:\n    - Variable Type (Base, Derived, Model)\n    - Calculation Status (Ready vs Needs Calculation)\n    - Warnings (Implicit computation, Auto-training, Inference)\n\n    Args:\n        target_variables: List of variable classes to explain.\n        legend: If True, print a legend for the warning codes.\n    \"\"\"\n    from varframe.variables import BaseVariable\n    from varframe.config import VFConfig, ImplicitOperation\n\n    # ANSI color codes\n    GREEN = \"\\033[92m\"\n    YELLOW = \"\\033[93m\"\n    PURPLE = \"\\033[95m\"\n    ORANGE = \"\\033[33m\"\n    GREY = \"\\033[90m\"\n    RESET = \"\\033[0m\"\n\n    resolved = resolve_dependencies(target_variables)\n\n    names = \", \".join(v.name for v in target_variables)\n    print(f\"Calculation Plan for {names}:\")\n\n    used_codes = set()\n    # Mapping: Code -&gt; (Description, ConfigCheck)\n    warning_defs = {\n        \"I\": (\"Implicit Compute\", VFConfig.warn_add_variable_compute),\n        \"T\": (\"Auto-Train Model\", VFConfig.warn_train_model),\n        \"M\": (\"Model Inference\", VFConfig.warn_infer_model),\n    }\n\n    for i, v in enumerate(resolved, 1):\n        # 1. Determine Type\n        if hasattr(v, \"model_class\") and v.model_class:\n            var_type = \"model_prediction\"\n            color = PURPLE\n        elif issubclass(v, BaseVariable):\n            var_type = \"base\"\n            color = GREEN\n        else:\n            var_type = \"derived\"\n            color = YELLOW\n\n        # 2. Determine Status\n        exists = v.name in self.columns\n        status_icon = \"\u2705\" if exists else \"\u23f3\"\n\n        # 3. Predict Warnings\n        current_codes = []\n        if not exists:\n            # Implicit Variable Warning\n            if v not in self._variables and warning_defs[\"I\"][1]:\n                 current_codes.append(\"I\")\n\n            # Model Warnings\n            if var_type == \"model_prediction\" and v.model_class:\n                if not v.model_class.is_trained and warning_defs[\"T\"][1]:\n                     current_codes.append(\"T\")\n                if warning_defs[\"M\"][1]:\n                    current_codes.append(\"M\")\n\n        used_codes.update(current_codes)\n\n        # Formatting\n        line = f\"  {i}. {status_icon} {color}{v.name}{RESET} ({var_type})\"\n        if current_codes:\n            codes_str = \",\".join(current_codes)\n            # Subtle grey for the warning hints\n            line += f\" {GREY}\u26a0 [{codes_str}]{RESET}\"\n\n        print(line)\n\n    if legend and used_codes:\n        print(\"\\nLegend:\")\n        for code in sorted(used_codes):\n            desc = warning_defs[code][0]\n            print(f\"  {GREY}\u26a0 [{code}]{RESET} : {desc}\")\n    print()\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.filter_by_type","title":"<code>filter_by_type(variable_type)</code>","text":"<p>Filter to include only columns of a specific variable type.</p> <p>Parameters:</p> Name Type Description Default <code>variable_type</code> <code>Type[Union[BaseVariable, DerivedVariable]]</code> <p>The base class to filter by (BaseVariable or DerivedVariable).</p> required <p>Returns:</p> Type Description <code>VarFrame</code> <p>A new VarFrame containing only columns whose variables</p> <code>VarFrame</code> <p>are subclasses of the specified type.</p> Example <p>derived_df = vf.filter_by_type(DerivedVariable) base_df = vf.filter_by_type(BaseVariable)</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def filter_by_type(\n    self, variable_type: Type[Union[BaseVariable, DerivedVariable]]\n) -&gt; VarFrame:\n    \"\"\"\n    Filter to include only columns of a specific variable type.\n\n    Args:\n        variable_type: The base class to filter by (BaseVariable or DerivedVariable).\n\n    Returns:\n        A new VarFrame containing only columns whose variables\n        are subclasses of the specified type.\n\n    Example:\n        &gt;&gt;&gt; derived_df = vf.filter_by_type(DerivedVariable)\n        &gt;&gt;&gt; base_df = vf.filter_by_type(BaseVariable)\n    \"\"\"\n    filtered_vars = [v for v in self._variables if issubclass(v, variable_type)]\n    filtered_names = [v.name for v in filtered_vars]\n    result = self[filtered_names].copy()\n    result._variables = filtered_vars\n    return result\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.from_pandas","title":"<code>from_pandas(df, variables=None, df_raw=None, name='varframe')</code>  <code>classmethod</code>","text":"<p>Create a VarFrame from a plain pandas DataFrame.</p> <p>Use this to re-wrap a DataFrame after ML operations or when loading data that was previously converted with to_pandas().</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to convert (already processed data).</p> required <code>variables</code> <code>Optional[VariableList]</code> <p>List of variable classes to associate with the DataFrame.</p> <code>None</code> <code>df_raw</code> <code>Optional[DataFrame]</code> <p>Optional raw DataFrame to store for future resolve() calls.</p> <code>None</code> <p>Returns:</p> Type Description <code>VarFrame</code> <p>A new VarFrame with the given data and variables.</p> Example <p>plain_df = pd.read_pickle(\"data.pkl\") vf = VarFrame.from_pandas(plain_df, variables=[Lap, Gap])</p> Source code in <code>varframe/dataframe.py</code> <pre><code>@classmethod\ndef from_pandas(\n    cls,\n    df: pd.DataFrame,\n    variables: Optional[VariableList] = None,\n    df_raw: Optional[pd.DataFrame] = None,\n    name: str = \"varframe\",\n) -&gt; VarFrame:\n    \"\"\"\n    Create a VarFrame from a plain pandas DataFrame.\n\n    Use this to re-wrap a DataFrame after ML operations or when\n    loading data that was previously converted with to_pandas().\n\n    Args:\n        df: The pandas DataFrame to convert (already processed data).\n        variables: List of variable classes to associate with the DataFrame.\n        df_raw: Optional raw DataFrame to store for future resolve() calls.\n\n    Returns:\n        A new VarFrame with the given data and variables.\n\n    Example:\n        &gt;&gt;&gt; plain_df = pd.read_pickle(\"data.pkl\")\n        &gt;&gt;&gt; vf = VarFrame.from_pandas(plain_df, variables=[Lap, Gap])\n    \"\"\"\n    vf = cls(df_raw=None, variables=None, compute=False, name=name)\n    for col in df.columns:\n        vf[col] = df[col].values\n    vf.index = df.index\n    vf._variables = list(variables) if variables else []\n    vf._df_raw = df_raw\n    return vf\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.get_variable","title":"<code>get_variable(name)</code>","text":"<p>Retrieve a variable class by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the variable to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[VariableType]</code> <p>The variable class if found, None otherwise.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def get_variable(self, name: str) -&gt; Optional[VariableType]:\n    \"\"\"\n    Retrieve a variable class by its name.\n\n    Args:\n        name: The name of the variable to retrieve.\n\n    Returns:\n        The variable class if found, None otherwise.\n    \"\"\"\n    for var in self._variables:\n        if var.name == name:\n            return var\n    return None\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.list_variables","title":"<code>list_variables()</code>","text":"<p>Get a list of all variable names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of variable names in order.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def list_variables(self) -&gt; List[str]:\n    \"\"\"\n    Get a list of all variable names.\n\n    Returns:\n        List of variable names in order.\n    \"\"\"\n    return [v.name for v in self._variables]\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.load_csv","title":"<code>load_csv(path_or_buf, variables=None, exclude=None, discard_unmatched=True, ambiguity=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load a VarFrame from a CSV file, automatically discovering variables.</p> <p>This method scans the current Python environment for variable definitions that match the columns in the CSV.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_buf</code> <code>Union[str, Any]</code> <p>Path to the CSV file.</p> required <code>variables</code> <code>Optional[List[VariableType]]</code> <p>Whitelist of variable classes to load. If provided, only  these variables will be matched. Acts as implicit disambiguation.</p> <code>None</code> <code>exclude</code> <code>Optional[List[VariableType]]</code> <p>Blacklist of variable classes to exclude from loading. Cannot be used together with <code>variables</code>.</p> <code>None</code> <code>discard_unmatched</code> <code>bool</code> <p>If True (default), columns not matching any known variable are dropped. If False, they are kept as plain columns.</p> <code>True</code> <code>ambiguity</code> <code>Optional[Dict[str, VariableType]]</code> <p>Dict mapping variable names to specific classes for  disambiguation when multiple definitions exist with the same name.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to pd.read_csv.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VarFrame</code> <p>A reconstructed VarFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>variables</code> and <code>exclude</code> are provided.</p> <code>AmbiguityError</code> <p>If multiple variable definitions match a column name and no disambiguation is provided.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>@classmethod\ndef load_csv(\n    cls, \n    path_or_buf: Union[str, Any], \n    variables: Optional[List[VariableType]] = None,\n    exclude: Optional[List[VariableType]] = None,\n    discard_unmatched: bool = True,\n    ambiguity: Optional[Dict[str, VariableType]] = None,\n    **kwargs: Any\n) -&gt; VarFrame:\n    \"\"\"\n    Load a VarFrame from a CSV file, automatically discovering variables.\n\n    This method scans the current Python environment for variable definitions\n    that match the columns in the CSV.\n\n    Args:\n        path_or_buf: Path to the CSV file.\n        variables: Whitelist of variable classes to load. If provided, only \n            these variables will be matched. Acts as implicit disambiguation.\n        exclude: Blacklist of variable classes to exclude from loading.\n            Cannot be used together with `variables`.\n        discard_unmatched: If True (default), columns not matching any known\n            variable are dropped. If False, they are kept as plain columns.\n        ambiguity: Dict mapping variable names to specific classes for \n            disambiguation when multiple definitions exist with the same name.\n        **kwargs: Arguments passed to pd.read_csv.\n\n    Returns:\n        A reconstructed VarFrame.\n\n    Raises:\n        ValueError: If both `variables` and `exclude` are provided.\n        AmbiguityError: If multiple variable definitions match a column name\n            and no disambiguation is provided.\n    \"\"\"\n    from varframe.exceptions import AmbiguityError\n\n    # Validation\n    if variables is not None and exclude is not None:\n        raise ValueError(\"Cannot use both 'variables' and 'exclude' parameters together.\")\n\n    df = pd.read_csv(path_or_buf, **kwargs)\n\n    # Auto-discovery\n    known_vars = cls._discover_all_variables()\n    matched_vars = []\n\n    # Build target names based on whitelist/blacklist\n    if variables is not None:\n        target_names = {v.name for v in variables}\n        # Create a lookup for whitelisted variables by name\n        whitelist_by_name: Dict[str, VariableType] = {v.name: v for v in variables}\n    elif exclude is not None:\n        exclude_names = {v.name for v in exclude}\n        target_names = {col for col in df.columns if col not in exclude_names}\n        whitelist_by_name = None\n    else:\n        target_names = set(df.columns)\n        whitelist_by_name = None\n\n    for col in df.columns:\n        if col not in target_names:\n            continue\n\n        if col not in known_vars:\n            continue\n\n        candidates = known_vars[col]\n\n        # If whitelist mode, use the whitelisted class directly\n        if whitelist_by_name is not None:\n            if col in whitelist_by_name:\n                matched_vars.append(whitelist_by_name[col])\n            continue\n\n        # Check for ambiguity\n        if len(candidates) &gt; 1:\n            # Check if disambiguation provided via ambiguity parameter\n            if ambiguity and col in ambiguity:\n                matched_vars.append(ambiguity[col])\n            else:\n                raise AmbiguityError(\n                    f\"Ambiguous variable '{col}'. Multiple definitions found: \"\n                    f\"{[c.__name__ for c in candidates]}. \"\n                    f\"Use 'ambiguity' parameter to specify which to use, e.g.: \"\n                    f\"ambiguity={{'{col}': {candidates[0].__name__}}}\"\n                )\n        else:\n            matched_vars.append(candidates[0])\n\n    # Handle unmatched columns\n    mapped_names = {v.name for v in matched_vars}\n    unmapped_cols = [col for col in df.columns if col not in mapped_names]\n\n    if discard_unmatched:\n        # Drop unmatched columns\n        cols_to_keep = [v.name for v in matched_vars]\n        df = df[[c for c in cols_to_keep if c in df.columns]]\n    else:\n        # Warn about unmapped columns (old behavior)\n        if unmapped_cols:\n            from varframe.config import VFConfig, ImplicitOperation\n            VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_NO_COMPUTE,\n                f\"Loaded columns {unmapped_cols} do not match any known Variable class. \"\n                \"They will be loaded as plain pandas columns.\"\n            )\n\n    # Try to infer name from path\n    name = \"varframe\"\n    if isinstance(path_or_buf, str):\n        import os\n        name = os.path.splitext(os.path.basename(path_or_buf))[0]\n\n    return cls.from_pandas(df, variables=matched_vars, name=name)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.load_parquet","title":"<code>load_parquet(path, variables=None, exclude=None, discard_unmatched=True, ambiguity=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load a VarFrame from a Parquet file, using metadata or auto-discovery.</p> <ol> <li>Checks for 'varframe_variables' metadata in the file.</li> <li>If found, looks up those variables in the environment.</li> <li>If not found, falls back to matching column names (like load_csv).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Any]</code> <p>Path to the Parquet file.</p> required <code>variables</code> <code>Optional[List[VariableType]]</code> <p>Whitelist of variable classes to load. If provided, only  these variables will be matched. Acts as implicit disambiguation and overrides hash-based resolution.</p> <code>None</code> <code>exclude</code> <code>Optional[List[VariableType]]</code> <p>Blacklist of variable classes to exclude from loading. Cannot be used together with <code>variables</code>.</p> <code>None</code> <code>discard_unmatched</code> <code>bool</code> <p>If True (default), columns not matching any known variable are dropped. If False, they are kept as plain columns.</p> <code>True</code> <code>ambiguity</code> <code>Optional[Dict[str, VariableType]]</code> <p>Dict mapping variable names to specific classes for  disambiguation. Overrides hash-based resolution for specified names.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to pyarrow/pandas read functions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VarFrame</code> <p>A reconstructed VarFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>variables</code> and <code>exclude</code> are provided.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>@classmethod\ndef load_parquet(\n    cls, \n    path: Union[str, Any], \n    variables: Optional[List[VariableType]] = None,\n    exclude: Optional[List[VariableType]] = None,\n    discard_unmatched: bool = True,\n    ambiguity: Optional[Dict[str, VariableType]] = None,\n    **kwargs: Any\n) -&gt; VarFrame:\n    \"\"\"\n    Load a VarFrame from a Parquet file, using metadata or auto-discovery.\n\n    1. Checks for 'varframe_variables' metadata in the file.\n    2. If found, looks up those variables in the environment.\n    3. If not found, falls back to matching column names (like load_csv).\n\n    Args:\n        path: Path to the Parquet file.\n        variables: Whitelist of variable classes to load. If provided, only \n            these variables will be matched. Acts as implicit disambiguation\n            and overrides hash-based resolution.\n        exclude: Blacklist of variable classes to exclude from loading.\n            Cannot be used together with `variables`.\n        discard_unmatched: If True (default), columns not matching any known\n            variable are dropped. If False, they are kept as plain columns.\n        ambiguity: Dict mapping variable names to specific classes for \n            disambiguation. Overrides hash-based resolution for specified names.\n        **kwargs: Arguments passed to pyarrow/pandas read functions.\n\n    Returns:\n        A reconstructed VarFrame.\n\n    Raises:\n        ValueError: If both `variables` and `exclude` are provided.\n    \"\"\"\n    import json\n\n    # Validation\n    if variables is not None and exclude is not None:\n        raise ValueError(\"Cannot use both 'variables' and 'exclude' parameters together.\")\n\n    # ANSI Colors\n    RED = \"\\033[91m\"\n    ORANGE = \"\\033[33m\"\n    WHITE = \"\\033[37m\"\n    RESET = \"\\033[0m\"\n\n    warnings_list = []\n\n    # Try reading metadata first (requires pyarrow)\n    file_hashes = {}\n    var_names_from_meta = []\n    all_columns = []\n\n    try:\n        import pyarrow.parquet as pq\n        meta = pq.read_metadata(path)\n        # Get column names from schema\n        all_columns = list(meta.schema.names)\n        if meta.metadata:\n            if b'varframe_variables' in meta.metadata:\n                var_names_from_meta = json.loads(meta.metadata[b'varframe_variables'])\n            if b'varframe_hashes' in meta.metadata:\n                file_hashes = json.loads(meta.metadata[b'varframe_hashes'])\n    except ImportError:\n        pass\n\n    known_vars = cls._discover_all_variables()\n\n    # Determine which columns to load based on parameters\n    if variables is not None:\n        # Whitelist mode - use provided classes directly\n        target_names = [v.name for v in variables]\n        whitelist_by_name: Dict[str, VariableType] = {v.name: v for v in variables}\n    elif exclude is not None:\n        exclude_names = {v.name for v in exclude}\n        source_names = var_names_from_meta if var_names_from_meta else all_columns\n        target_names = [n for n in source_names if n not in exclude_names]\n        whitelist_by_name = None\n    else:\n        target_names = None  # Load all\n        whitelist_by_name = None\n\n    # Use pyarrow column selection for efficiency\n    try:\n        import pyarrow.parquet as pq\n        if target_names is not None:\n            # Only read requested columns\n            cols_to_read = [c for c in target_names if c in all_columns]\n            table = pq.read_table(path, columns=cols_to_read, **kwargs)\n        else:\n            table = pq.read_table(path, **kwargs)\n        df = table.to_pandas()\n    except ImportError:\n        # Fallback to pandas (less efficient)\n        df = pd.read_parquet(path, **kwargs)\n        if target_names is not None:\n            available = [c for c in target_names if c in df.columns]\n            df = df[available]\n\n    matched_vars = []\n\n    # Determine variable names to look for\n    if target_names is not None:\n        names_to_resolve = target_names\n    else:\n        names_to_resolve = var_names_from_meta if var_names_from_meta else df.columns\n\n    for name in names_to_resolve:\n        # Handle case where name is in metadata but not in known_vars (not imported)\n        if name not in known_vars:\n            continue\n\n        # If whitelist mode, use the whitelisted class directly\n        if whitelist_by_name is not None:\n            if name in whitelist_by_name:\n                matched_vars.append(whitelist_by_name[name])\n            continue\n\n        candidates = known_vars[name]\n\n        # Check for multiple candidates (ambiguity)\n        if len(candidates) &gt; 1:\n            # Require explicit disambiguation\n            if ambiguity and name in ambiguity:\n                matched_vars.append(ambiguity[name])\n            else:\n                from varframe.exceptions import AmbiguityError\n                raise AmbiguityError(\n                    f\"Ambiguous variable '{name}'. Multiple definitions found: \"\n                    f\"{[c.__name__ for c in candidates]}. \"\n                    f\"Use 'ambiguity' parameter to specify which to use, e.g.: \"\n                    f\"ambiguity={{'{name}': {candidates[0].__name__}}}\"\n                )\n        else:\n            # Single candidate - use it\n            matched_vars.append(candidates[0])\n\n    # Handle unmatched columns\n    mapped_names = {v.name for v in matched_vars}\n    unmapped_cols = [col for col in df.columns if col not in mapped_names]\n\n    if discard_unmatched:\n        # Drop unmatched columns\n        cols_to_keep = [v.name for v in matched_vars]\n        df = df[[c for c in cols_to_keep if c in df.columns]]\n    else:\n        # Warn about unmapped columns (old behavior)\n        if unmapped_cols:\n            from varframe.config import VFConfig, ImplicitOperation\n            VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_NO_COMPUTE,\n                f\"Loaded columns {unmapped_cols} do not match any known Variable class. \"\n                \"They will be loaded as plain pandas columns.\"\n            )\n\n    # Try to infer name from path\n    vf_name = \"varframe\"\n    if isinstance(path, str):\n        import os\n        vf_name = os.path.splitext(os.path.basename(path))[0]\n\n    # ------------------- Integrity Check -------------------\n    # Compare loaded variables against current environment\n\n    if file_hashes:\n        for var in matched_vars:\n            if var.name not in file_hashes:\n                continue\n\n            stored = file_hashes[var.name]\n            current = var.get_hash_components()\n\n            # Check for changes\n            diffs = []\n            severity = 0 # 0=None, 1=White, 2=Orange, 3=Red\n\n            # 1. Calculation (RED)\n            if stored.get(\"calc\") != current[\"calc\"]:\n                diffs.append(\"Calculation logic changed\")\n                severity = max(severity, 3)\n\n            # 2. Dependencies (RED)\n            if stored.get(\"deps\") != current[\"deps\"]:\n                diffs.append(\"Dependencies changed\")\n                severity = max(severity, 3)\n\n            # 3. Attributes (ORANGE) - dtype\n            if stored.get(\"attrs\") != current[\"attrs\"]:\n                diffs.append(\"Attributes (dtype) changed\")\n                severity = max(severity, 2)\n\n            # 4. Metadata (WHITE) - description, lazy\n            if stored.get(\"meta\") != current[\"meta\"]:\n                diffs.append(\"Metadata (desc/lazy) changed\")\n                severity = max(severity, 1)\n\n            if diffs:\n                color = WHITE\n                if severity == 3: color = RED\n                elif severity == 2: color = ORANGE\n\n                msg = f\"{color}Variable '{var.name}': {', '.join(diffs)}{RESET}\"\n                warnings_list.append(msg)\n\n    if warnings_list:\n        print(f\"\\n{ORANGE}VarFrame Integrity Check Warnings:{RESET}\")\n        for w in warnings_list:\n            print(f\"  {w}\")\n\n    return cls.from_pandas(df, variables=matched_vars, name=vf_name)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.resolve","title":"<code>resolve(*target_variables, suppress_warnings=False)</code>","text":"<p>Resolve and compute target variables with automatic dependency resolution.</p> <p>This method automatically determines all missing dependencies for the target variables, computes them in the correct DAG order, and adds them to the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*target_variables</code> <code>VariableType</code> <p>Variable classes to resolve and compute.</p> <code>()</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress warnings for this call only.</p> <code>False</code> <p>Returns:</p> Type Description <code>VarFrame</code> <p>Self, for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If circular dependencies are detected.</p> <code>ValueError</code> <p>If a BaseVariable is needed but df_raw is not available.</p> <code>RuntimeError</code> <p>If implicit operations are disabled in VFConfig.</p> Example <p>vf = VarFrame(df_raw, [Lap, Gap]) vf.resolve(PredictedGapDelta)</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def resolve(\n    self,\n    *target_variables: VariableType,\n    suppress_warnings: bool = False,\n) -&gt; VarFrame:\n    \"\"\"\n    Resolve and compute target variables with automatic dependency resolution.\n\n    This method automatically determines all missing dependencies for the\n    target variables, computes them in the correct DAG order, and adds\n    them to the DataFrame.\n\n    Args:\n        *target_variables: Variable classes to resolve and compute.\n        suppress_warnings: If True, suppress warnings for this call only.\n\n    Returns:\n        Self, for method chaining.\n\n    Raises:\n        ValueError: If circular dependencies are detected.\n        ValueError: If a BaseVariable is needed but df_raw is not available.\n        RuntimeError: If implicit operations are disabled in VFConfig.\n\n    Example:\n        &gt;&gt;&gt; vf = VarFrame(df_raw, [Lap, Gap])\n        &gt;&gt;&gt; vf.resolve(PredictedGapDelta)\n    \"\"\"\n    # Resolve all dependencies\n    all_vars = resolve_dependencies(list(target_variables))\n\n    # Filter to only missing variables\n    # Filter to only missing variables\n    missing_vars = [v for v in all_vars if v.name not in self.columns]\n\n    # Split into compute-now vs lazy\n    lazy_vars = [\n        v for v in missing_vars if issubclass(v, DerivedVariable) and v.lazy\n    ]\n    missing_vars = [v for v in missing_vars if v not in lazy_vars]\n\n    # Check for missing BaseVariables - need df_raw to compute them\n    missing_base = [v for v in missing_vars if issubclass(v, BaseVariable)]\n    if missing_base and self._df_raw is None:\n        names = [v.name for v in missing_base]\n        raise ValueError(\n            f\"Cannot resolve BaseVariables {names}: no raw DataFrame stored. \"\n            \"Pass df_raw to VarFrame() or set vf.df_raw to enable.\"\n        )\n\n    # Warn about implicit computation of missing variables\n    if missing_vars and not suppress_warnings:\n        var_names = [v.name for v in missing_vars]\n        VFConfig.check_permission(\n            ImplicitOperation.ADD_VARIABLE_COMPUTE,\n            f\"Cannot resolve variables {var_names}. \"\n            \"Set VFConfig.allow_implicit_compute = True to enable.\",\n        )\n        VFConfig.warn(\n            ImplicitOperation.ADD_VARIABLE_COMPUTE,\n            f\"Implicitly computing {len(missing_vars)} variable(s) not in _variables: {var_names}\",\n            stacklevel=3,\n        )\n\n    # Register lazy variables (so get_variable works)\n    for var in lazy_vars:\n        if var not in self._variables:\n            self._variables.append(var)\n\n    if suppress_warnings:\n        context = VFConfig.suppress_warnings()\n    else:\n        context = VFConfig.null_context()\n\n    with context:\n        # Compute missing variables in order\n        for var in missing_vars:\n            if issubclass(var, BaseVariable):\n                self[var.name] = var.compute(self._df_raw)\n            else:\n                self[var.name] = var.compute(self)\n\n            if var not in self._variables:\n                self._variables.append(var)\n\n    return self\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.to_csv","title":"<code>to_csv(path_or_buf=None, *args, include=None, variables=None, **kwargs)</code>","text":"<p>Write object to a comma-separated values (csv) file.</p> <p>Enhancements over pandas.to_csv: - Supports <code>include</code> and <code>variables</code> to compute lazy variables on-the-fly. - Warns if registered variables are not included in the export. - Defaults to {self.name}.csv if path is not provided.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def to_csv(\n    self,\n    path_or_buf: Optional[Union[str, Any]] = None,\n    *args: Any,\n    include: Optional[List[str]] = None,\n    variables: Optional[VariableList] = None,\n    **kwargs: Any,\n) -&gt; Optional[str]:\n    \"\"\"\n    Write object to a comma-separated values (csv) file.\n\n    Enhancements over pandas.to_csv:\n    - Supports `include` and `variables` to compute lazy variables on-the-fly.\n    - Warns if registered variables are not included in the export.\n    - Defaults to {self.name}.csv if path is not provided.\n    \"\"\"\n    from varframe.config import VFConfig\n\n    # 1. Resolve Data\n    if include or variables:\n        df_to_export = self.view(include=include, variables=variables)\n    else:\n        df_to_export = self\n\n    # 2. Prevent implicit data loss (Warn if variables are missing)\n    # Check which variables are in the export\n    exported_cols = set(df_to_export.columns)\n    missing_vars = [\n        v.name for v in self._variables \n        if v.name not in exported_cols and v.name not in self.columns\n    ]\n\n    # Also check for variables present in self but not in export (if view filtered them)\n    excluded_present_vars = [\n         v.name for v in self._variables\n         if v.name in self.columns and v.name not in exported_cols\n    ]\n\n    if missing_vars:\n         path_str = str(path_or_buf) if path_or_buf else \"output\"\n         VFConfig.warn(\n            ImplicitOperation.ADD_VARIABLE_COMPUTE, # Reusing generic warning op\n            f\"Exporting to {path_str} without computing lazy variables: {missing_vars}. \"\n            \"Use `include=['all']` or `variables=[...]` to compute them during export.\",\n         )\n\n    # 3. Resolve Path\n    if path_or_buf is None:\n        path_or_buf = f\"{self.name}.csv\"\n\n    if hasattr(df_to_export, \"to_pandas\"):\n        return df_to_export.to_pandas().to_csv(path_or_buf, *args, **kwargs)\n    else:\n        return df_to_export.to_csv(path_or_buf, *args, **kwargs)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.to_ml","title":"<code>to_ml()</code>","text":"<p>Alias for to_pandas(). Explicit conversion for ML pipelines.</p> <p>Use this to clearly indicate the DataFrame is being prepared for machine learning operations.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A plain pandas DataFrame suitable for ML libraries.</p> Example <p>X_train = vf.to_ml() model.fit(X_train, y_train)</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def to_ml(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Alias for to_pandas(). Explicit conversion for ML pipelines.\n\n    Use this to clearly indicate the DataFrame is being prepared\n    for machine learning operations.\n\n    Returns:\n        A plain pandas DataFrame suitable for ML libraries.\n\n    Example:\n        &gt;&gt;&gt; X_train = vf.to_ml()\n        &gt;&gt;&gt; model.fit(X_train, y_train)\n    \"\"\"\n    return pd.DataFrame(self)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Convert to a plain pandas DataFrame.</p> <p>Use this before passing to ML libraries that may have issues with DataFrame subclasses (e.g., pickle, joblib, some sklearn pipelines).</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A plain pandas DataFrame with the same data (no variable metadata).</p> Example <p>plain_df = vf.to_pandas() model.fit(plain_df, y)</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert to a plain pandas DataFrame.\n\n    Use this before passing to ML libraries that may have issues\n    with DataFrame subclasses (e.g., pickle, joblib, some sklearn pipelines).\n\n    Returns:\n        A plain pandas DataFrame with the same data (no variable metadata).\n\n    Example:\n        &gt;&gt;&gt; plain_df = vf.to_pandas()\n        &gt;&gt;&gt; model.fit(plain_df, y)\n    \"\"\"\n    return pd.DataFrame(self)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.to_parquet","title":"<code>to_parquet(path=None, *args, include=None, variables=None, **kwargs)</code>","text":"<p>Write object to a binary parquet file.</p> <p>Enhancements over pandas.to_parquet: - Supports <code>include</code> and <code>variables</code> to compute lazy variables on-the-fly. - Warns if registered variables are not included in the export. - Defaults to {self.name}.parquet if path is not provided. - Saves variable names in file metadata.</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def to_parquet(\n    self,\n    path: Optional[Union[str, Any]] = None,\n    *args: Any,\n    include: Optional[List[str]] = None,\n    variables: Optional[VariableList] = None,\n    **kwargs: Any,\n) -&gt; Optional[bytes]:\n    \"\"\"\n    Write object to a binary parquet file.\n\n    Enhancements over pandas.to_parquet:\n    - Supports `include` and `variables` to compute lazy variables on-the-fly.\n    - Warns if registered variables are not included in the export.\n    - Defaults to {self.name}.parquet if path is not provided.\n    - Saves variable names in file metadata.\n    \"\"\"\n    import json\n    from varframe.config import VFConfig\n\n    # 1. Resolve Data\n    if include or variables:\n        df_to_export = self.view(include=include, variables=variables)\n    else:\n        df_to_export = self\n\n    # 2. Prevent implicit data loss\n    exported_cols = set(df_to_export.columns)\n    missing_vars = [\n        v.name for v in self._variables \n        if v.name not in exported_cols and v.name not in self.columns\n    ]\n\n    if missing_vars:\n         path_str = str(path) if path else \"output\"\n         VFConfig.warn(\n            ImplicitOperation.ADD_VARIABLE_COMPUTE,\n            f\"Exporting to {path_str} without computing lazy variables: {missing_vars}. \"\n            \"Use `include=['all']` or `variables=[...]` to compute them during export.\",\n         )\n\n    # 3. Resolve Path\n    if path is None:\n        path = f\"{self.name}.parquet\"\n\n    # 4. Prepare Metadata (Strategy B)\n    # Get existing keyword args or creating new dict\n    # pyarrow.Table.from_pandas uses 'preserve_index' etc.\n    # to_parquet kwargs are passed to the engine. \n    # For pyarrow engine (default), we can't easily inject metadata via to_parquet directly \n    # because pandas abstracts this. \n    # WORKAROUND: We will use table properties if using pyarrow, \n    # but to keep it simple and pandas-compliant, we might need a distinct approach.\n    # Actually, pandas &gt;= 1.0 does not easily support custom metadata in to_parquet \n    # without dropping down to pyarrow directly.\n\n    try:\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n\n        # Convert to Table\n        # Handle VarFrame or plain DataFrame\n        if hasattr(df_to_export, \"to_pandas\"):\n            df_plain = df_to_export.to_pandas()\n        else:\n            df_plain = df_to_export\n\n        table = pa.Table.from_pandas(df_plain)\n\n        # Add Metadata\n        custom_meta = {\n            \"varframe_variables\": json.dumps([v.name for v in self._variables])\n        }\n\n        # Add Hash Metadata\n        try:\n            hashes = {v.name: v.get_hash_components() for v in self._variables}\n            custom_meta[\"varframe_hashes\"] = json.dumps(hashes)\n        except Exception as e:\n            VFConfig.warn(\n                ImplicitOperation.ADD_VARIABLE_COMPUTE,\n                f\"Failed to compute variable hashes for export: {e}\"\n            )\n        # Combine with existing metadata\n        existing_meta = table.schema.metadata or {}\n        combined_meta = {**existing_meta, **{k.encode(): v.encode() for k, v in custom_meta.items()}}\n        table = table.replace_schema_metadata(combined_meta)\n\n        # Write\n        pq.write_table(table, path, *args, **kwargs)\n        return None\n\n    except ImportError:\n        # Fallback for when pyarrow is not available or user wants different engine\n         VFConfig.warn(\n             ImplicitOperation.ADD_VARIABLE_COMPUTE,\n             \"Pyarrow not found or failed. Exporting without VarFrame metadata.\"\n         )\n         if hasattr(df_to_export, \"to_pandas\"):\n             return df_to_export.to_pandas().to_parquet(path, *args, **kwargs)\n         else:\n             return df_to_export.to_parquet(path, *args, **kwargs)\n</code></pre>"},{"location":"api_reference/#varframe.VarFrame.view","title":"<code>view(include=None, variables=None)</code>","text":"<p>Create a DataFrame view containing only specific variables.</p> <p>Parameters:</p> Name Type Description Default <code>include</code> <code>Optional[List[str]]</code> <p>List of variable categories to include. Options: 'base', 'derived', 'lazy', 'model', 'all'. Defaults to ['all'] if both include and variables are None.</p> <code>None</code> <code>variables</code> <code>Optional[VariableList]</code> <p>Explicit list of variable classes to include.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with the requested variables. </p> <code>DataFrame</code> <p>Lazy variables will be computed on-demand for this view.</p> Example <p>vf.view(include=['base', 'lazy']) vf.view(variables=[LazySum])</p> Source code in <code>varframe/dataframe.py</code> <pre><code>def view(\n    self,\n    include: Optional[List[str]] = None,\n    variables: Optional[VariableList] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a DataFrame view containing only specific variables.\n\n    Args:\n        include: List of variable categories to include.\n            Options: 'base', 'derived', 'lazy', 'model', 'all'.\n            Defaults to ['all'] if both include and variables are None.\n        variables: Explicit list of variable classes to include.\n\n    Returns:\n        A pandas DataFrame with the requested variables. \n        Lazy variables will be computed on-demand for this view.\n\n    Example:\n        &gt;&gt;&gt; vf.view(include=['base', 'lazy'])\n        &gt;&gt;&gt; vf.view(variables=[LazySum])\n    \"\"\"\n    target_vars = []\n\n    # 1. Handle 'variables' argument\n    if variables:\n        target_vars.extend(variables)\n\n    # 2. Handle 'include' argument\n    if include:\n        for category in include:\n            if category == \"all\":\n                target_vars.extend(self._variables)\n            elif category == \"base\":\n                target_vars.extend(\n                    [v for v in self._variables if issubclass(v, BaseVariable)]\n                )\n            elif category == \"derived\":\n                # Non-lazy derived\n                target_vars.extend(\n                    [\n                        v\n                        for v in self._variables\n                        if issubclass(v, DerivedVariable) and not v.lazy\n                    ]\n                )\n            elif category == \"lazy\":\n                # Lazy derived\n                target_vars.extend(\n                    [\n                        v\n                        for v in self._variables\n                        if issubclass(v, DerivedVariable) and v.lazy\n                    ]\n                )\n            elif category == \"model\":\n                target_vars.extend(\n                    [\n                        v\n                        for v in self._variables\n                        if hasattr(v, \"model_class\") and v.model_class\n                    ]\n                )\n\n    # Default to 'all' if nothing specified\n    if not target_vars:\n        target_vars = list(self._variables)\n\n    # Remove duplicates while preserving order\n    target_vars = list(dict.fromkeys(target_vars))\n\n    # 3. Construct DataFrame\n    data = {}\n    for var in target_vars:\n        # If standard column, use it (zero copy if possible)\n        if var.name in self.columns:\n            data[var.name] = self[var.name]\n        else:\n            # Must be lazy or missing -&gt; Compute it\n            data[var.name] = var.compute(self)\n\n    return pd.DataFrame(data, index=self.index)\n</code></pre>"},{"location":"api_reference/#variables","title":"Variables","text":""},{"location":"api_reference/#varframe.BaseVariable","title":"<code>varframe.BaseVariable</code>","text":"<p>A declarative variable that maps to a column in a raw DataFrame.</p> <p>Define subclasses with class attributes to create variable definitions. The class itself represents the variable - no instantiation needed.</p> Class Attributes <p>name (str): The name of the variable in the processed DataFrame. raw_column (str): The column name in the raw DataFrame to extract. dtype (str): The pandas dtype to cast the column to. Defaults to 'float'.</p> Note <p>Use the class docstring as the variable description.</p> Example <p>class LapNumber(BaseVariable): ...     '''The current lap number in the race.''' ...     name = \"lap\" ...     raw_column = \"lap_num\" ...     dtype = \"int\" ...</p> Source code in <code>varframe/variables.py</code> <pre><code>class BaseVariable:\n    \"\"\"\n    A declarative variable that maps to a column in a raw DataFrame.\n\n    Define subclasses with class attributes to create variable definitions.\n    The class itself represents the variable - no instantiation needed.\n\n    Class Attributes:\n        **name (str)**: The name of the variable in the processed DataFrame.\n        **raw_column (str)**: The column name in the raw DataFrame to extract.\n        **dtype (str)**: The pandas dtype to cast the column to. Defaults to 'float'.\n\n    Note:\n        Use the class docstring as the variable description.\n\n    Example:\n        &gt;&gt;&gt; class LapNumber(BaseVariable):\n        ...     '''The current lap number in the race.'''\n        ...     name = \"lap\"\n        ...     raw_column = \"lap_num\"\n        ...     dtype = \"int\"\n        ...\n        &gt;&gt;&gt; # Use the class directly, not an instance\n        &gt;&gt;&gt; series = LapNumber.compute(raw_dataframe)\n    \"\"\"\n\n    # Class attributes to be overridden by subclasses\n    name: ClassVar[str] = \"\"\n    raw_column: ClassVar[str] = \"\"\n    dtype: ClassVar[str] = \"float\"\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"Validate subclass definition on class creation.\"\"\"\n        super().__init_subclass__(**kwargs)\n        # Auto-generate name from class name if not specified\n        if not cls.name and cls.__name__ not in (\"BaseVariable\", \"DerivedVariable\"):\n            cls.name = cls.__name__.lower()\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        \"\"\"\n        Get the variable description from the class docstring.\n\n        Returns:\n            The first line of the class docstring, or empty string if none.\n        \"\"\"\n        if cls.__doc__:\n            return cls.__doc__.strip().split(\"\\n\")[0]\n        return \"\"\n\n    @classmethod\n    def compute(cls, df_raw: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"\n        Extract and transform the column from a raw DataFrame.\n\n        Args:\n            df_raw: The raw DataFrame containing the source column.\n\n        Returns:\n            A pandas Series with the extracted column cast to the specified dtype.\n\n        Raises:\n            KeyError: If `raw_column` does not exist in `df_raw`.\n            ValueError: If the column cannot be cast to the specified `dtype`.\n        \"\"\"\n        return df_raw[cls.raw_column].astype(cls.dtype)\n\n    @classmethod\n    def get_hash_components(cls) -&gt; Dict[str, str]:\n        \"\"\"\n        Get the hash components of the variable definition.\n\n        Returns:\n            Dict with keys: calc, deps, attrs, meta.\n        \"\"\"\n        # Calc: raw_column\n        calc_hash = hashlib.sha256(cls.raw_column.encode(\"utf-8\")).hexdigest()\n\n        # Deps: None for BaseVariable\n        deps_hash = hashlib.sha256(b\"\").hexdigest()\n\n        # Attrs: dtype\n        attrs_hash = hashlib.sha256(cls.dtype.encode(\"utf-8\")).hexdigest()\n\n        # Meta: description\n        meta_hash = hashlib.sha256(cls.get_description().encode(\"utf-8\")).hexdigest()\n\n        return {\n            \"calc\": calc_hash,\n            \"deps\": deps_hash,\n            \"attrs\": attrs_hash,\n            \"meta\": meta_hash,\n        }\n\n    @classmethod\n    def info(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get variable metadata as a dictionary.\n\n        Returns:\n            Dict with name, type, raw_column, dtype, and description.\n        \"\"\"\n        return {\n            \"name\": cls.name,\n            \"type\": \"base\",\n            \"raw_column\": cls.raw_column,\n            \"dtype\": cls.dtype,\n            \"description\": cls.get_description(),\n        }\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}: {self.name}&gt;\"\n</code></pre>"},{"location":"api_reference/#varframe.BaseVariable--use-the-class-directly-not-an-instance","title":"Use the class directly, not an instance","text":"<p>series = LapNumber.compute(raw_dataframe)</p>"},{"location":"api_reference/#varframe.BaseVariable.compute","title":"<code>compute(df_raw)</code>  <code>classmethod</code>","text":"<p>Extract and transform the column from a raw DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_raw</code> <code>DataFrame</code> <p>The raw DataFrame containing the source column.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas Series with the extracted column cast to the specified dtype.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>raw_column</code> does not exist in <code>df_raw</code>.</p> <code>ValueError</code> <p>If the column cannot be cast to the specified <code>dtype</code>.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef compute(cls, df_raw: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Extract and transform the column from a raw DataFrame.\n\n    Args:\n        df_raw: The raw DataFrame containing the source column.\n\n    Returns:\n        A pandas Series with the extracted column cast to the specified dtype.\n\n    Raises:\n        KeyError: If `raw_column` does not exist in `df_raw`.\n        ValueError: If the column cannot be cast to the specified `dtype`.\n    \"\"\"\n    return df_raw[cls.raw_column].astype(cls.dtype)\n</code></pre>"},{"location":"api_reference/#varframe.BaseVariable.get_hash_components","title":"<code>get_hash_components()</code>  <code>classmethod</code>","text":"<p>Get the hash components of the variable definition.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict with keys: calc, deps, attrs, meta.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef get_hash_components(cls) -&gt; Dict[str, str]:\n    \"\"\"\n    Get the hash components of the variable definition.\n\n    Returns:\n        Dict with keys: calc, deps, attrs, meta.\n    \"\"\"\n    # Calc: raw_column\n    calc_hash = hashlib.sha256(cls.raw_column.encode(\"utf-8\")).hexdigest()\n\n    # Deps: None for BaseVariable\n    deps_hash = hashlib.sha256(b\"\").hexdigest()\n\n    # Attrs: dtype\n    attrs_hash = hashlib.sha256(cls.dtype.encode(\"utf-8\")).hexdigest()\n\n    # Meta: description\n    meta_hash = hashlib.sha256(cls.get_description().encode(\"utf-8\")).hexdigest()\n\n    return {\n        \"calc\": calc_hash,\n        \"deps\": deps_hash,\n        \"attrs\": attrs_hash,\n        \"meta\": meta_hash,\n    }\n</code></pre>"},{"location":"api_reference/#varframe.BaseVariable.info","title":"<code>info()</code>  <code>classmethod</code>","text":"<p>Get variable metadata as a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with name, type, raw_column, dtype, and description.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef info(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get variable metadata as a dictionary.\n\n    Returns:\n        Dict with name, type, raw_column, dtype, and description.\n    \"\"\"\n    return {\n        \"name\": cls.name,\n        \"type\": \"base\",\n        \"raw_column\": cls.raw_column,\n        \"dtype\": cls.dtype,\n        \"description\": cls.get_description(),\n    }\n</code></pre>"},{"location":"api_reference/#varframe.DerivedVariable","title":"<code>varframe.DerivedVariable</code>","text":"<p>A declarative variable computed from other variables.</p> <p>Define subclasses with class attributes and a <code>calculate()</code> classmethod to create computed variable definitions. Dependencies are resolved automatically using the DataFrame as the single source of truth.</p> Class Attributes <p>name (str): The name of the variable in the processed DataFrame. dependencies (List[Type]): List of variable classes this depends on. dtype (str): The pandas dtype for the result. Defaults to 'float'. lazy (bool): If True, computed on access and not stored in DataFrame. Defaults to False.</p> Note <ul> <li>Use the class docstring as the variable description.</li> <li>Override the <code>calculate()</code> classmethod to define computation logic.</li> <li>Access dependencies directly from df (e.g., <code>df[\"gap\"]</code>), not memo.</li> </ul> Example <p>class GapDelta(DerivedVariable): ...     '''Change in gap from previous measurement.''' ...     name = \"gap_delta\" ...     dependencies = [Gap] ... ...     @classmethod ...     def calculate(cls, df: pd.DataFrame) -&gt; pd.Series: ...         return df[\"gap\"] - df[\"gap\"].shift(1)</p> Source code in <code>varframe/variables.py</code> <pre><code>class DerivedVariable:\n    \"\"\"\n    A declarative variable computed from other variables.\n\n    Define subclasses with class attributes and a `calculate()` classmethod\n    to create computed variable definitions. Dependencies are resolved\n    automatically using the DataFrame as the single source of truth.\n\n    Class Attributes:\n        name (str): The name of the variable in the processed DataFrame.\n        dependencies (List[Type]): List of variable classes this depends on.\n        dtype (str): The pandas dtype for the result. Defaults to 'float'.\n        lazy (bool): If True, computed on access and not stored in DataFrame. Defaults to False.\n\n    Note:\n        - Use the class docstring as the variable description.\n        - Override the `calculate()` classmethod to define computation logic.\n        - Access dependencies directly from df (e.g., `df[\"gap\"]`), not memo.\n\n    Example:\n        &gt;&gt;&gt; class GapDelta(DerivedVariable):\n        ...     '''Change in gap from previous measurement.'''\n        ...     name = \"gap_delta\"\n        ...     dependencies = [Gap]\n        ...\n        ...     @classmethod\n        ...     def calculate(cls, df: pd.DataFrame) -&gt; pd.Series:\n        ...         return df[\"gap\"] - df[\"gap\"].shift(1)\n    \"\"\"\n\n    # Class attributes to be overridden by subclasses\n    name: ClassVar[str] = \"\"\n    dependencies: ClassVar[List[\"VariableType\"]] = []\n    dtype: ClassVar[str] = \"float\"\n    lazy: ClassVar[bool] = False\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"Validate subclass definition and set defaults.\"\"\"\n        super().__init_subclass__(**kwargs)\n        # Auto-generate name from class name if not specified\n        if not cls.name and cls.__name__ not in (\"BaseVariable\", \"DerivedVariable\"):\n            cls.name = cls.__name__.lower()\n        # Ensure dependencies is a new list (not shared reference)\n        if \"dependencies\" not in cls.__dict__:\n            cls.dependencies = []\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        \"\"\"\n        Get the variable description from the class docstring.\n\n        Returns:\n            The first line of the class docstring, or empty string if none.\n        \"\"\"\n        if cls.__doc__:\n            return cls.__doc__.strip().split(\"\\n\")[0]\n        return \"\"\n\n    @classmethod\n    def calculate(cls, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"\n        Calculate the derived variable's values.\n\n        Override this method in subclasses to define the computation logic.\n        Access dependency values directly from df (e.g., `df[\"gap\"]`).\n\n        Args:\n            df: The current processed DataFrame containing all computed\n                variables so far (including dependencies).\n\n        Returns:\n            A pandas Series with the computed values.\n\n        Raises:\n            NotImplementedError: If not overridden in subclass.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{cls.__name__} must implement the 'calculate' classmethod\"\n        )\n\n    @classmethod\n    def compute(cls, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"\n        Compute the derived variable from the DataFrame.\n\n        Dependencies must already exist in df before calling this method.\n        Use `VarFrame` to handle dependency ordering automatically.\n\n        Args:\n            df: The DataFrame containing all dependency columns.\n\n        Returns:\n            A pandas Series containing the computed values.\n\n        Raises:\n            KeyError: If a dependency column is missing from df.\n        \"\"\"\n        # Verify dependencies exist in df\n        for dep in cls.dependencies:\n            # If dependency is in columns, we are good\n            if dep.name in df.columns:\n                continue\n\n            # If dependency is Lazy, we assume it can be computed on-demand by __getitem__\n            # so we don't raise KeyError here.\n            is_lazy_dep = getattr(dep, \"lazy\", False)\n            if is_lazy_dep:\n                continue\n\n            raise KeyError(\n                f\"Dependency '{dep.name}' not found in DataFrame. \"\n                f\"Ensure dependencies are computed before '{cls.name}'.\"\n            )\n        return cls.calculate(df)\n\n    @classmethod\n    def get_hash_components(cls) -&gt; Dict[str, str]:\n        \"\"\"\n        Get the hash components of the variable definition.\n\n        Returns:\n            Dict with keys: calc, deps, attrs, meta.\n        \"\"\"\n        # Calc: Source code of calculate method\n        try:\n            source = inspect.getsource(cls.calculate)\n        except (OSError, TypeError):\n            source = \"\"\n        calc_hash = hashlib.sha256(source.encode(\"utf-8\")).hexdigest()\n\n        # Deps: Recursive hash of dependencies\n        # We combine the full hashes of all dependencies\n        dep_hashes = []\n        for dep in cls.dependencies:\n            # Recursively get hash (we use a flattened version generally, \n            # but here we just need a signature of the dependency state).\n            # To avoid infinite recursion in malformed cyclic graphs (though resolve handles that),\n            # we rely on the fact that dependencies must be solved variables.\n            # Ideally, we want the hash of the dependency variable itself.\n\n            # Note: A dependency change should ripple up. \n            # We use the dependency's class name + its own component hashes\n            try:\n                d_comps = dep.get_hash_components()\n                # Encapsulate strictly functional components for dependency hash\n                # We exclude 'meta' so that docstring changes don't invalidate downstream calculations\n                functional_comps = {k: v for k, v in d_comps.items() if k != \"meta\"}\n                d_combined = hashlib.sha256(json.dumps(functional_comps, sort_keys=True).encode(\"utf-8\")).hexdigest()\n                dep_hashes.append(f\"{dep.__name__}:{d_combined}\")\n            except Exception:\n                # If dependency is broken or not a Variable class\n                dep_hashes.append(f\"{dep}:{str(dep)}\")\n\n        deps_str = \",\".join(sorted(dep_hashes))\n        deps_hash = hashlib.sha256(deps_str.encode(\"utf-8\")).hexdigest()\n\n        # Attrs: dtype\n        attrs_hash = hashlib.sha256(cls.dtype.encode(\"utf-8\")).hexdigest()\n\n        # Meta: description, lazy\n        meta_str = f\"{cls.get_description()}|{cls.lazy}\"\n        meta_hash = hashlib.sha256(meta_str.encode(\"utf-8\")).hexdigest()\n\n        return {\n            \"calc\": calc_hash,\n            \"deps\": deps_hash,\n            \"attrs\": attrs_hash,\n            \"meta\": meta_hash,\n        }\n\n    @classmethod\n    def info(cls) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get variable metadata as a dictionary.\n\n        Returns:\n            Dict with name, type, dependencies, dtype, and description.\n        \"\"\"\n        return {\n            \"name\": cls.name,\n            \"type\": \"derived\",\n            \"dependencies\": [d.name for d in cls.dependencies],\n            \"dtype\": cls.dtype,\n            \"lazy\": cls.lazy,\n            \"description\": cls.get_description(),\n        }\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}: {self.name}&gt;\"\n</code></pre>"},{"location":"api_reference/#varframe.DerivedVariable.calculate","title":"<code>calculate(df)</code>  <code>classmethod</code>","text":"<p>Calculate the derived variable's values.</p> <p>Override this method in subclasses to define the computation logic. Access dependency values directly from df (e.g., <code>df[\"gap\"]</code>).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The current processed DataFrame containing all computed variables so far (including dependencies).</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas Series with the computed values.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not overridden in subclass.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef calculate(cls, df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Calculate the derived variable's values.\n\n    Override this method in subclasses to define the computation logic.\n    Access dependency values directly from df (e.g., `df[\"gap\"]`).\n\n    Args:\n        df: The current processed DataFrame containing all computed\n            variables so far (including dependencies).\n\n    Returns:\n        A pandas Series with the computed values.\n\n    Raises:\n        NotImplementedError: If not overridden in subclass.\n    \"\"\"\n    raise NotImplementedError(\n        f\"{cls.__name__} must implement the 'calculate' classmethod\"\n    )\n</code></pre>"},{"location":"api_reference/#varframe.DerivedVariable.compute","title":"<code>compute(df)</code>  <code>classmethod</code>","text":"<p>Compute the derived variable from the DataFrame.</p> <p>Dependencies must already exist in df before calling this method. Use <code>VarFrame</code> to handle dependency ordering automatically.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing all dependency columns.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas Series containing the computed values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a dependency column is missing from df.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef compute(cls, df: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"\n    Compute the derived variable from the DataFrame.\n\n    Dependencies must already exist in df before calling this method.\n    Use `VarFrame` to handle dependency ordering automatically.\n\n    Args:\n        df: The DataFrame containing all dependency columns.\n\n    Returns:\n        A pandas Series containing the computed values.\n\n    Raises:\n        KeyError: If a dependency column is missing from df.\n    \"\"\"\n    # Verify dependencies exist in df\n    for dep in cls.dependencies:\n        # If dependency is in columns, we are good\n        if dep.name in df.columns:\n            continue\n\n        # If dependency is Lazy, we assume it can be computed on-demand by __getitem__\n        # so we don't raise KeyError here.\n        is_lazy_dep = getattr(dep, \"lazy\", False)\n        if is_lazy_dep:\n            continue\n\n        raise KeyError(\n            f\"Dependency '{dep.name}' not found in DataFrame. \"\n            f\"Ensure dependencies are computed before '{cls.name}'.\"\n        )\n    return cls.calculate(df)\n</code></pre>"},{"location":"api_reference/#varframe.DerivedVariable.get_hash_components","title":"<code>get_hash_components()</code>  <code>classmethod</code>","text":"<p>Get the hash components of the variable definition.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict with keys: calc, deps, attrs, meta.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef get_hash_components(cls) -&gt; Dict[str, str]:\n    \"\"\"\n    Get the hash components of the variable definition.\n\n    Returns:\n        Dict with keys: calc, deps, attrs, meta.\n    \"\"\"\n    # Calc: Source code of calculate method\n    try:\n        source = inspect.getsource(cls.calculate)\n    except (OSError, TypeError):\n        source = \"\"\n    calc_hash = hashlib.sha256(source.encode(\"utf-8\")).hexdigest()\n\n    # Deps: Recursive hash of dependencies\n    # We combine the full hashes of all dependencies\n    dep_hashes = []\n    for dep in cls.dependencies:\n        # Recursively get hash (we use a flattened version generally, \n        # but here we just need a signature of the dependency state).\n        # To avoid infinite recursion in malformed cyclic graphs (though resolve handles that),\n        # we rely on the fact that dependencies must be solved variables.\n        # Ideally, we want the hash of the dependency variable itself.\n\n        # Note: A dependency change should ripple up. \n        # We use the dependency's class name + its own component hashes\n        try:\n            d_comps = dep.get_hash_components()\n            # Encapsulate strictly functional components for dependency hash\n            # We exclude 'meta' so that docstring changes don't invalidate downstream calculations\n            functional_comps = {k: v for k, v in d_comps.items() if k != \"meta\"}\n            d_combined = hashlib.sha256(json.dumps(functional_comps, sort_keys=True).encode(\"utf-8\")).hexdigest()\n            dep_hashes.append(f\"{dep.__name__}:{d_combined}\")\n        except Exception:\n            # If dependency is broken or not a Variable class\n            dep_hashes.append(f\"{dep}:{str(dep)}\")\n\n    deps_str = \",\".join(sorted(dep_hashes))\n    deps_hash = hashlib.sha256(deps_str.encode(\"utf-8\")).hexdigest()\n\n    # Attrs: dtype\n    attrs_hash = hashlib.sha256(cls.dtype.encode(\"utf-8\")).hexdigest()\n\n    # Meta: description, lazy\n    meta_str = f\"{cls.get_description()}|{cls.lazy}\"\n    meta_hash = hashlib.sha256(meta_str.encode(\"utf-8\")).hexdigest()\n\n    return {\n        \"calc\": calc_hash,\n        \"deps\": deps_hash,\n        \"attrs\": attrs_hash,\n        \"meta\": meta_hash,\n    }\n</code></pre>"},{"location":"api_reference/#varframe.DerivedVariable.info","title":"<code>info()</code>  <code>classmethod</code>","text":"<p>Get variable metadata as a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with name, type, dependencies, dtype, and description.</p> Source code in <code>varframe/variables.py</code> <pre><code>@classmethod\ndef info(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get variable metadata as a dictionary.\n\n    Returns:\n        Dict with name, type, dependencies, dtype, and description.\n    \"\"\"\n    return {\n        \"name\": cls.name,\n        \"type\": \"derived\",\n        \"dependencies\": [d.name for d in cls.dependencies],\n        \"dtype\": cls.dtype,\n        \"lazy\": cls.lazy,\n        \"description\": cls.get_description(),\n    }\n</code></pre>"},{"location":"api_reference/#machine-learning","title":"Machine Learning","text":""},{"location":"api_reference/#varframe.BaseModel","title":"<code>varframe.BaseModel</code>","text":"<p>A declarative base class for defining ML models.</p> <p>Define subclasses with class attributes to specify input features, target variable, model type, and hyperparameters.</p> Class Attributes <p>name (str): Unique identifier for the model. input_vars (List[VariableType]): Variables used as features (X). target_var (VariableType): Variable to predict (y). model_class (Type): The model class (e.g., RandomForestRegressor). model_params (Dict): Hyperparameters passed to model_class(). model (Any): The trained model instance (set after training). is_trained (bool): Whether the model has been trained.</p> Example <p>class GapPredictor(BaseModel): ...     name = \"gap_predictor\" ...     input_vars = [Lap, Gap, TireAge] ...     target_var = GapDelta ...     model_class = RandomForestRegressor ...     model_params = {\"n_estimators\": 100}</p> Source code in <code>varframe/models.py</code> <pre><code>class BaseModel:\n    \"\"\"\n    A declarative base class for defining ML models.\n\n    Define subclasses with class attributes to specify input features,\n    target variable, model type, and hyperparameters.\n\n    Class Attributes:\n        name (str): Unique identifier for the model.\n        input_vars (List[VariableType]): Variables used as features (X).\n        target_var (VariableType): Variable to predict (y).\n        model_class (Type): The model class (e.g., RandomForestRegressor).\n        model_params (Dict): Hyperparameters passed to model_class().\n        model (Any): The trained model instance (set after training).\n        is_trained (bool): Whether the model has been trained.\n\n    Example:\n        &gt;&gt;&gt; class GapPredictor(BaseModel):\n        ...     name = \"gap_predictor\"\n        ...     input_vars = [Lap, Gap, TireAge]\n        ...     target_var = GapDelta\n        ...     model_class = RandomForestRegressor\n        ...     model_params = {\"n_estimators\": 100}\n    \"\"\"\n\n    name: ClassVar[str] = \"\"\n    input_vars: ClassVar[List[VariableType]] = []\n    target_var: ClassVar[Optional[VariableType]] = None\n    model_class: ClassVar[Optional[Type[Any]]] = None\n    model_params: ClassVar[Dict[str, Any]] = {}\n    model: ClassVar[Any] = None\n    is_trained: ClassVar[bool] = False\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize subclass with defaults.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if not cls.name and cls.__name__ != \"BaseModel\":\n            cls.name = cls.__name__.lower()\n        if \"input_vars\" not in cls.__dict__:\n            cls.input_vars = []\n        if \"model_params\" not in cls.__dict__:\n            cls.model_params = {}\n        cls.model = None\n        cls.is_trained = False\n\n    @classmethod\n    def get_description(cls) -&gt; str:\n        \"\"\"Get model description from class docstring.\"\"\"\n        if cls.__doc__:\n            return cls.__doc__.strip().split(\"\\n\")[0]\n        return \"\"\n\n    @classmethod\n    def get_input_names(cls) -&gt; List[str]:\n        \"\"\"Get list of input variable names.\"\"\"\n        return [v.name for v in cls.input_vars]\n\n    @classmethod\n    def get_target_name(cls) -&gt; Optional[str]:\n        \"\"\"Get target variable name.\"\"\"\n        return cls.target_var.name if cls.target_var else None\n\n    @classmethod\n    def prepare_X(cls, vf: Union[\"VarFrame\", pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"Extract input features from a DataFrame.\"\"\"\n        input_names = cls.get_input_names()\n        missing = [n for n in input_names if n not in vf.columns]\n        if missing:\n            raise KeyError(f\"Missing input variables: {missing}\")\n\n        X = vf[input_names]\n        if hasattr(X, \"to_ml\"):\n            X = X.to_ml()\n        return X\n\n    @classmethod\n    def prepare_y(cls, vf: Union[\"VarFrame\", pd.DataFrame]) -&gt; pd.Series:\n        \"\"\"Extract target variable from a DataFrame.\"\"\"\n        if cls.target_var is None:\n            raise ValueError(f\"{cls.__name__} has no target_var defined\")\n\n        target_name = cls.get_target_name()\n        if target_name not in vf.columns:\n            raise KeyError(f\"Missing target variable: {target_name}\")\n\n        return vf[target_name]\n\n    @classmethod\n    def train(\n        cls,\n        vf: Union[\"VarFrame\", pd.DataFrame],\n        **fit_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Train the model on the provided data.\"\"\"\n        if cls.model_class is None:\n            raise ValueError(\n                f\"{cls.__name__} must define 'model_class'. \"\n                \"Example: model_class = RandomForestRegressor\"\n            )\n\n        cls.model = cls.model_class(**cls.model_params)\n        X = cls.prepare_X(vf)\n        y = cls.prepare_y(vf)\n        cls.model.fit(X, y, **fit_kwargs)\n        cls.is_trained = True\n\n    @classmethod\n    def train_with_validation(\n        cls,\n        train_vf: Union[\"VarFrame\", pd.DataFrame],\n        val_vf: Union[\"VarFrame\", pd.DataFrame],\n        **fit_kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Train with a validation set (for XGBoost, LightGBM, etc.).\"\"\"\n        if cls.model_class is None:\n            raise ValueError(f\"{cls.__name__} must define 'model_class'.\")\n\n        cls.model = cls.model_class(**cls.model_params)\n        X_train = cls.prepare_X(train_vf)\n        y_train = cls.prepare_y(train_vf)\n        X_val = cls.prepare_X(val_vf)\n        y_val = cls.prepare_y(val_vf)\n\n        fit_params = fit_kwargs.copy()\n        model_name = cls.model_class.__name__.lower()\n\n        if any(name in model_name for name in [\"xgb\", \"lgbm\", \"lightgbm\", \"catboost\"]):\n            fit_params.setdefault(\"eval_set\", [(X_val, y_val)])\n\n        cls.model.fit(X_train, y_train, **fit_params)\n        cls.is_trained = True\n\n    @classmethod\n    def predict(\n        cls,\n        vf: Union[\"VarFrame\", pd.DataFrame],\n        add_to_df: bool = True,\n        column_name: Optional[str] = None,\n    ) -&gt; pd.Series:\n        \"\"\"Generate predictions.\"\"\"\n        if not cls.is_trained or cls.model is None:\n            raise ValueError(f\"{cls.__name__} must be trained before calling predict()\")\n\n        X = cls.prepare_X(vf)\n        predictions = cls.model.predict(X)\n\n        col_name = column_name or f\"{cls.name}_pred\"\n        result = pd.Series(predictions, index=vf.index, name=col_name)\n\n        if add_to_df and hasattr(vf, \"_variables\"):\n            vf[col_name] = result\n\n        return result\n\n    @classmethod\n    def evaluate(\n        cls,\n        vf: Union[\"VarFrame\", pd.DataFrame],\n        metrics: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, float]:\n        \"\"\"Evaluate model performance.\"\"\"\n        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n        if metrics is None:\n            metrics = {\n                \"mse\": mean_squared_error,\n                \"mae\": mean_absolute_error,\n                \"r2\": r2_score,\n            }\n\n        y_true = cls.prepare_y(vf)\n        y_pred = cls.predict(vf, add_to_df=False)\n\n        results = {}\n        for name, metric_fn in metrics.items():\n            mask = ~(y_true.isna() | pd.Series(y_pred).isna())\n            results[name] = metric_fn(y_true[mask], y_pred[mask])\n\n        return results\n\n    @classmethod\n    def info(cls) -&gt; Dict[str, Any]:\n        \"\"\"Get model metadata as a dictionary.\"\"\"\n        return {\n            \"name\": cls.name,\n            \"description\": cls.get_description(),\n            \"input_vars\": cls.get_input_names(),\n            \"target_var\": cls.get_target_name(),\n            \"model_class\": cls.model_class.__name__ if cls.model_class else None,\n            \"model_params\": cls.model_params,\n            \"is_trained\": cls.is_trained,\n        }\n\n    @classmethod\n    def save(cls, path: str) -&gt; None:\n        \"\"\"Save the trained model to disk.\"\"\"\n        import joblib\n\n        if not cls.is_trained:\n            raise ValueError(f\"{cls.__name__} must be trained before saving\")\n\n        joblib.dump(\n            {\n                \"model\": cls.model,\n                \"input_vars\": cls.get_input_names(),\n                \"target_var\": cls.get_target_name(),\n                \"name\": cls.name,\n            },\n            path,\n        )\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        \"\"\"Load a trained model from disk.\"\"\"\n        import joblib\n\n        data = joblib.load(path)\n        cls.model = data[\"model\"]\n        cls.is_trained = True\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}: {self.name} (trained={self.is_trained})&gt;\"\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.evaluate","title":"<code>evaluate(vf, metrics=None)</code>  <code>classmethod</code>","text":"<p>Evaluate model performance.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef evaluate(\n    cls,\n    vf: Union[\"VarFrame\", pd.DataFrame],\n    metrics: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model performance.\"\"\"\n    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n    if metrics is None:\n        metrics = {\n            \"mse\": mean_squared_error,\n            \"mae\": mean_absolute_error,\n            \"r2\": r2_score,\n        }\n\n    y_true = cls.prepare_y(vf)\n    y_pred = cls.predict(vf, add_to_df=False)\n\n    results = {}\n    for name, metric_fn in metrics.items():\n        mask = ~(y_true.isna() | pd.Series(y_pred).isna())\n        results[name] = metric_fn(y_true[mask], y_pred[mask])\n\n    return results\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.get_input_names","title":"<code>get_input_names()</code>  <code>classmethod</code>","text":"<p>Get list of input variable names.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef get_input_names(cls) -&gt; List[str]:\n    \"\"\"Get list of input variable names.\"\"\"\n    return [v.name for v in cls.input_vars]\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.get_target_name","title":"<code>get_target_name()</code>  <code>classmethod</code>","text":"<p>Get target variable name.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef get_target_name(cls) -&gt; Optional[str]:\n    \"\"\"Get target variable name.\"\"\"\n    return cls.target_var.name if cls.target_var else None\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.info","title":"<code>info()</code>  <code>classmethod</code>","text":"<p>Get model metadata as a dictionary.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef info(cls) -&gt; Dict[str, Any]:\n    \"\"\"Get model metadata as a dictionary.\"\"\"\n    return {\n        \"name\": cls.name,\n        \"description\": cls.get_description(),\n        \"input_vars\": cls.get_input_names(),\n        \"target_var\": cls.get_target_name(),\n        \"model_class\": cls.model_class.__name__ if cls.model_class else None,\n        \"model_params\": cls.model_params,\n        \"is_trained\": cls.is_trained,\n    }\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a trained model from disk.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    \"\"\"Load a trained model from disk.\"\"\"\n    import joblib\n\n    data = joblib.load(path)\n    cls.model = data[\"model\"]\n    cls.is_trained = True\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.predict","title":"<code>predict(vf, add_to_df=True, column_name=None)</code>  <code>classmethod</code>","text":"<p>Generate predictions.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef predict(\n    cls,\n    vf: Union[\"VarFrame\", pd.DataFrame],\n    add_to_df: bool = True,\n    column_name: Optional[str] = None,\n) -&gt; pd.Series:\n    \"\"\"Generate predictions.\"\"\"\n    if not cls.is_trained or cls.model is None:\n        raise ValueError(f\"{cls.__name__} must be trained before calling predict()\")\n\n    X = cls.prepare_X(vf)\n    predictions = cls.model.predict(X)\n\n    col_name = column_name or f\"{cls.name}_pred\"\n    result = pd.Series(predictions, index=vf.index, name=col_name)\n\n    if add_to_df and hasattr(vf, \"_variables\"):\n        vf[col_name] = result\n\n    return result\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.save","title":"<code>save(path)</code>  <code>classmethod</code>","text":"<p>Save the trained model to disk.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef save(cls, path: str) -&gt; None:\n    \"\"\"Save the trained model to disk.\"\"\"\n    import joblib\n\n    if not cls.is_trained:\n        raise ValueError(f\"{cls.__name__} must be trained before saving\")\n\n    joblib.dump(\n        {\n            \"model\": cls.model,\n            \"input_vars\": cls.get_input_names(),\n            \"target_var\": cls.get_target_name(),\n            \"name\": cls.name,\n        },\n        path,\n    )\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.train","title":"<code>train(vf, **fit_kwargs)</code>  <code>classmethod</code>","text":"<p>Train the model on the provided data.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef train(\n    cls,\n    vf: Union[\"VarFrame\", pd.DataFrame],\n    **fit_kwargs: Any,\n) -&gt; None:\n    \"\"\"Train the model on the provided data.\"\"\"\n    if cls.model_class is None:\n        raise ValueError(\n            f\"{cls.__name__} must define 'model_class'. \"\n            \"Example: model_class = RandomForestRegressor\"\n        )\n\n    cls.model = cls.model_class(**cls.model_params)\n    X = cls.prepare_X(vf)\n    y = cls.prepare_y(vf)\n    cls.model.fit(X, y, **fit_kwargs)\n    cls.is_trained = True\n</code></pre>"},{"location":"api_reference/#varframe.BaseModel.train_with_validation","title":"<code>train_with_validation(train_vf, val_vf, **fit_kwargs)</code>  <code>classmethod</code>","text":"<p>Train with a validation set (for XGBoost, LightGBM, etc.).</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef train_with_validation(\n    cls,\n    train_vf: Union[\"VarFrame\", pd.DataFrame],\n    val_vf: Union[\"VarFrame\", pd.DataFrame],\n    **fit_kwargs: Any,\n) -&gt; None:\n    \"\"\"Train with a validation set (for XGBoost, LightGBM, etc.).\"\"\"\n    if cls.model_class is None:\n        raise ValueError(f\"{cls.__name__} must define 'model_class'.\")\n\n    cls.model = cls.model_class(**cls.model_params)\n    X_train = cls.prepare_X(train_vf)\n    y_train = cls.prepare_y(train_vf)\n    X_val = cls.prepare_X(val_vf)\n    y_val = cls.prepare_y(val_vf)\n\n    fit_params = fit_kwargs.copy()\n    model_name = cls.model_class.__name__.lower()\n\n    if any(name in model_name for name in [\"xgb\", \"lgbm\", \"lightgbm\", \"catboost\"]):\n        fit_params.setdefault(\"eval_set\", [(X_val, y_val)])\n\n    cls.model.fit(X_train, y_train, **fit_params)\n    cls.is_trained = True\n</code></pre>"},{"location":"api_reference/#varframe.ModelVariable","title":"<code>varframe.ModelVariable</code>","text":"<p>               Bases: <code>DerivedVariable</code></p> <p>A derived variable computed via model inference.</p> Class Attributes <p>name (str): The name of the prediction variable. model_class (Type[BaseModel]): The model class to use for predictions. dependencies: Auto-populated from model's input_vars.</p> Example <p>class PredictedGap(ModelVariable): ...     name = \"predicted_gap\" ...     model_class = GapPredictor</p> Source code in <code>varframe/models.py</code> <pre><code>class ModelVariable(DerivedVariable):\n    \"\"\"\n    A derived variable computed via model inference.\n\n    Class Attributes:\n        name (str): The name of the prediction variable.\n        model_class (Type[BaseModel]): The model class to use for predictions.\n        dependencies: Auto-populated from model's input_vars.\n\n    Example:\n        &gt;&gt;&gt; class PredictedGap(ModelVariable):\n        ...     name = \"predicted_gap\"\n        ...     model_class = GapPredictor\n    \"\"\"\n\n    model_class: ClassVar[Optional[Type[BaseModel]]] = None\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize and validate ModelVariable subclass.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        if cls.model_class is not None:\n            cls.dependencies = list(cls.model_class.input_vars)\n\n    @classmethod\n    def calculate(\n        cls,\n        df: pd.DataFrame,\n        suppress_warnings: bool = False,\n    ) -&gt; pd.Series:\n        \"\"\"\n        Calculate predictions using the associated model.\n\n        If the model is not trained, it will be automatically trained\n        using the available data in the DataFrame (with a warning).\n        \"\"\"\n        if cls.model_class is None:\n            raise ValueError(f\"{cls.__name__} must define 'model_class' attribute\")\n\n        model_name = cls.model_class.name\n\n        # Auto-train if model is not trained\n        if not cls.model_class.is_trained:\n            target_name = (\n                cls.model_class.target_var.name\n                if cls.model_class.target_var\n                else \"unknown\"\n            )\n\n            if cls.model_class.target_var is None:\n                raise ValueError(\n                    f\"Cannot auto-train model '{model_name}': no target_var defined\"\n                )\n\n            if target_name not in df.columns:\n                raise ValueError(\n                    f\"Cannot auto-train model '{model_name}': \"\n                    f\"target variable '{target_name}' not in DataFrame.\"\n                )\n\n            VFConfig.check_permission(\n                ImplicitOperation.TRAIN_MODEL,\n                f\"Cannot auto-train model '{model_name}'. \"\n                \"Set VFConfig.allow_implicit_train = True to enable.\",\n            )\n\n            if not suppress_warnings:\n                VFConfig.warn(\n                    ImplicitOperation.TRAIN_MODEL,\n                    f\"Auto-training model '{model_name}' (target: {target_name}).\",\n                    stacklevel=4,\n                )\n\n            train_data = df.dropna()\n            cls.model_class.train(train_data)\n\n        # Check permission and warn for inference\n        VFConfig.check_permission(\n            ImplicitOperation.INFER_MODEL,\n            f\"Cannot perform inference with model '{model_name}'.\",\n        )\n\n        if not suppress_warnings:\n            VFConfig.warn(\n                ImplicitOperation.INFER_MODEL,\n                f\"Performing inference with model '{model_name}'.\",\n                stacklevel=4,\n            )\n\n        return cls.model_class.predict(df, add_to_df=False)\n\n    @classmethod\n    def info(cls) -&gt; Dict[str, Any]:\n        \"\"\"Get variable metadata including model info.\"\"\"\n        base_info = super().info()\n        base_info[\"model\"] = cls.model_class.name if cls.model_class else None\n        base_info[\"type\"] = \"model_prediction\"\n        return base_info\n</code></pre>"},{"location":"api_reference/#varframe.ModelVariable.calculate","title":"<code>calculate(df, suppress_warnings=False)</code>  <code>classmethod</code>","text":"<p>Calculate predictions using the associated model.</p> <p>If the model is not trained, it will be automatically trained using the available data in the DataFrame (with a warning).</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef calculate(\n    cls,\n    df: pd.DataFrame,\n    suppress_warnings: bool = False,\n) -&gt; pd.Series:\n    \"\"\"\n    Calculate predictions using the associated model.\n\n    If the model is not trained, it will be automatically trained\n    using the available data in the DataFrame (with a warning).\n    \"\"\"\n    if cls.model_class is None:\n        raise ValueError(f\"{cls.__name__} must define 'model_class' attribute\")\n\n    model_name = cls.model_class.name\n\n    # Auto-train if model is not trained\n    if not cls.model_class.is_trained:\n        target_name = (\n            cls.model_class.target_var.name\n            if cls.model_class.target_var\n            else \"unknown\"\n        )\n\n        if cls.model_class.target_var is None:\n            raise ValueError(\n                f\"Cannot auto-train model '{model_name}': no target_var defined\"\n            )\n\n        if target_name not in df.columns:\n            raise ValueError(\n                f\"Cannot auto-train model '{model_name}': \"\n                f\"target variable '{target_name}' not in DataFrame.\"\n            )\n\n        VFConfig.check_permission(\n            ImplicitOperation.TRAIN_MODEL,\n            f\"Cannot auto-train model '{model_name}'. \"\n            \"Set VFConfig.allow_implicit_train = True to enable.\",\n        )\n\n        if not suppress_warnings:\n            VFConfig.warn(\n                ImplicitOperation.TRAIN_MODEL,\n                f\"Auto-training model '{model_name}' (target: {target_name}).\",\n                stacklevel=4,\n            )\n\n        train_data = df.dropna()\n        cls.model_class.train(train_data)\n\n    # Check permission and warn for inference\n    VFConfig.check_permission(\n        ImplicitOperation.INFER_MODEL,\n        f\"Cannot perform inference with model '{model_name}'.\",\n    )\n\n    if not suppress_warnings:\n        VFConfig.warn(\n            ImplicitOperation.INFER_MODEL,\n            f\"Performing inference with model '{model_name}'.\",\n            stacklevel=4,\n        )\n\n    return cls.model_class.predict(df, add_to_df=False)\n</code></pre>"},{"location":"api_reference/#varframe.ModelVariable.info","title":"<code>info()</code>  <code>classmethod</code>","text":"<p>Get variable metadata including model info.</p> Source code in <code>varframe/models.py</code> <pre><code>@classmethod\ndef info(cls) -&gt; Dict[str, Any]:\n    \"\"\"Get variable metadata including model info.\"\"\"\n    base_info = super().info()\n    base_info[\"model\"] = cls.model_class.name if cls.model_class else None\n    base_info[\"type\"] = \"model_prediction\"\n    return base_info\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry","title":"<code>varframe.ModelRegistry</code>","text":"<p>Central registry for managing multiple models.</p> Example <p>registry = ModelRegistry() registry.register(GapPredictor) registry.train_all(training_vf) registry.save_all(\"models/\")</p> Source code in <code>varframe/models.py</code> <pre><code>class ModelRegistry:\n    \"\"\"\n    Central registry for managing multiple models.\n\n    Example:\n        &gt;&gt;&gt; registry = ModelRegistry()\n        &gt;&gt;&gt; registry.register(GapPredictor)\n        &gt;&gt;&gt; registry.train_all(training_vf)\n        &gt;&gt;&gt; registry.save_all(\"models/\")\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an empty registry.\"\"\"\n        self._models: Dict[str, Type[BaseModel]] = {}\n\n    def register(self, model_class: Type[BaseModel]) -&gt; None:\n        \"\"\"Register a model class.\"\"\"\n        self._models[model_class.name] = model_class\n\n    def get(self, name: str) -&gt; Optional[Type[BaseModel]]:\n        \"\"\"Get a model class by name.\"\"\"\n        return self._models.get(name)\n\n    def list_models(self) -&gt; List[str]:\n        \"\"\"Get list of registered model names.\"\"\"\n        return list(self._models.keys())\n\n    def train_all(\n        self,\n        vf: Union[\"VarFrame\", pd.DataFrame],\n        models: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, bool]:\n        \"\"\"Train all registered models.\"\"\"\n        models = models or {}\n        results = {}\n\n        for name, model_class in self._models.items():\n            try:\n                model_class.train(vf)\n                results[name] = True\n            except Exception as e:\n                print(f\"Failed to train {name}: {e}\")\n                results[name] = False\n\n        return results\n\n    def evaluate_all(\n        self, vf: Union[\"VarFrame\", pd.DataFrame]\n    ) -&gt; Dict[str, Dict[str, float]]:\n        \"\"\"Evaluate all trained models.\"\"\"\n        results = {}\n        for name, model_class in self._models.items():\n            if model_class.is_trained:\n                results[name] = model_class.evaluate(vf)\n        return results\n\n    def save_all(self, directory: str) -&gt; None:\n        \"\"\"Save all trained models to a directory.\"\"\"\n        import os\n\n        os.makedirs(directory, exist_ok=True)\n\n        for name, model_class in self._models.items():\n            if model_class.is_trained:\n                path = os.path.join(directory, f\"{name}.joblib\")\n                model_class.save(path)\n\n    def load_all(self, directory: str) -&gt; None:\n        \"\"\"Load all models from a directory.\"\"\"\n        import os\n\n        for name, model_class in self._models.items():\n            path = os.path.join(directory, f\"{name}.joblib\")\n            if os.path.exists(path):\n                model_class.load(path)\n\n    def describe(self) -&gt; pd.DataFrame:\n        \"\"\"Generate a summary DataFrame of all registered models.\"\"\"\n        data = [model_class.info() for model_class in self._models.values()]\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.describe","title":"<code>describe()</code>","text":"<p>Generate a summary DataFrame of all registered models.</p> Source code in <code>varframe/models.py</code> <pre><code>def describe(self) -&gt; pd.DataFrame:\n    \"\"\"Generate a summary DataFrame of all registered models.\"\"\"\n    data = [model_class.info() for model_class in self._models.values()]\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.evaluate_all","title":"<code>evaluate_all(vf)</code>","text":"<p>Evaluate all trained models.</p> Source code in <code>varframe/models.py</code> <pre><code>def evaluate_all(\n    self, vf: Union[\"VarFrame\", pd.DataFrame]\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"Evaluate all trained models.\"\"\"\n    results = {}\n    for name, model_class in self._models.items():\n        if model_class.is_trained:\n            results[name] = model_class.evaluate(vf)\n    return results\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.get","title":"<code>get(name)</code>","text":"<p>Get a model class by name.</p> Source code in <code>varframe/models.py</code> <pre><code>def get(self, name: str) -&gt; Optional[Type[BaseModel]]:\n    \"\"\"Get a model class by name.\"\"\"\n    return self._models.get(name)\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.list_models","title":"<code>list_models()</code>","text":"<p>Get list of registered model names.</p> Source code in <code>varframe/models.py</code> <pre><code>def list_models(self) -&gt; List[str]:\n    \"\"\"Get list of registered model names.\"\"\"\n    return list(self._models.keys())\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.load_all","title":"<code>load_all(directory)</code>","text":"<p>Load all models from a directory.</p> Source code in <code>varframe/models.py</code> <pre><code>def load_all(self, directory: str) -&gt; None:\n    \"\"\"Load all models from a directory.\"\"\"\n    import os\n\n    for name, model_class in self._models.items():\n        path = os.path.join(directory, f\"{name}.joblib\")\n        if os.path.exists(path):\n            model_class.load(path)\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.register","title":"<code>register(model_class)</code>","text":"<p>Register a model class.</p> Source code in <code>varframe/models.py</code> <pre><code>def register(self, model_class: Type[BaseModel]) -&gt; None:\n    \"\"\"Register a model class.\"\"\"\n    self._models[model_class.name] = model_class\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.save_all","title":"<code>save_all(directory)</code>","text":"<p>Save all trained models to a directory.</p> Source code in <code>varframe/models.py</code> <pre><code>def save_all(self, directory: str) -&gt; None:\n    \"\"\"Save all trained models to a directory.\"\"\"\n    import os\n\n    os.makedirs(directory, exist_ok=True)\n\n    for name, model_class in self._models.items():\n        if model_class.is_trained:\n            path = os.path.join(directory, f\"{name}.joblib\")\n            model_class.save(path)\n</code></pre>"},{"location":"api_reference/#varframe.ModelRegistry.train_all","title":"<code>train_all(vf, models=None)</code>","text":"<p>Train all registered models.</p> Source code in <code>varframe/models.py</code> <pre><code>def train_all(\n    self,\n    vf: Union[\"VarFrame\", pd.DataFrame],\n    models: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, bool]:\n    \"\"\"Train all registered models.\"\"\"\n    models = models or {}\n    results = {}\n\n    for name, model_class in self._models.items():\n        try:\n            model_class.train(vf)\n            results[name] = True\n        except Exception as e:\n            print(f\"Failed to train {name}: {e}\")\n            results[name] = False\n\n    return results\n</code></pre>"},{"location":"api_reference/#dependencies","title":"Dependencies","text":""},{"location":"api_reference/#varframe.resolve_dependencies","title":"<code>varframe.resolve_dependencies(target_variables, include_model_training_deps=True)</code>","text":"<p>Resolve all dependencies for the given variables using DAG traversal.</p> <p>Performs a topological sort to determine the correct computation order, ensuring all dependencies are computed before their dependents.</p> <p>Parameters:</p> Name Type Description Default <code>target_variables</code> <code>'VariableList'</code> <p>List of variable classes to compute.</p> required <code>include_model_training_deps</code> <code>bool</code> <p>If True, includes dependencies needed for model training (target_var of models).</p> <code>True</code> <p>Returns:</p> Type Description <code>'VariableList'</code> <p>A topologically sorted list of all variables needed to compute</p> <code>'VariableList'</code> <p>the target variables, with dependencies before dependents.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a circular dependency is detected.</p> Example Source code in <code>varframe/dependencies.py</code> <pre><code>def resolve_dependencies(\n    target_variables: \"VariableList\",\n    include_model_training_deps: bool = True,\n) -&gt; \"VariableList\":\n    \"\"\"\n    Resolve all dependencies for the given variables using DAG traversal.\n\n    Performs a topological sort to determine the correct computation order,\n    ensuring all dependencies are computed before their dependents.\n\n    Args:\n        target_variables: List of variable classes to compute.\n        include_model_training_deps: If True, includes dependencies needed\n            for model training (target_var of models).\n\n    Returns:\n        A topologically sorted list of all variables needed to compute\n        the target variables, with dependencies before dependents.\n\n    Raises:\n        ValueError: If a circular dependency is detected.\n\n    Example:\n        &gt;&gt;&gt; # Only specify final variables - dependencies auto-resolved!\n        &gt;&gt;&gt; all_vars = resolve_dependencies([PredictedGapDelta])\n        &gt;&gt;&gt; # Returns: [Lap, Gap, TireAge, GapDelta, PredictedGapDelta]\n    \"\"\"\n    # Collect all dependencies recursively\n    all_vars: Set[\"VariableType\"] = set()\n    visiting: Set[\"VariableType\"] = set()  # For cycle detection\n\n    def collect(var: \"VariableType\") -&gt; None:\n        \"\"\"Recursively collect all dependencies for a variable.\"\"\"\n        if var in all_vars:\n            return\n        if var in visiting:\n            raise ValueError(f\"Circular dependency detected involving '{var.name}'\")\n\n        visiting.add(var)\n\n        # Get direct dependencies\n        deps: List[\"VariableType\"] = []\n\n        # DerivedVariable dependencies\n        if hasattr(var, \"dependencies\") and var.dependencies:\n            deps.extend(var.dependencies)\n\n        # ModelVariable: also needs model's input_vars for prediction\n        # and target_var for training (if include_model_training_deps)\n        if hasattr(var, \"model_class\") and var.model_class is not None:\n            model_cls = var.model_class\n            if hasattr(model_cls, \"input_vars\") and model_cls.input_vars:\n                deps.extend(model_cls.input_vars)\n            if include_model_training_deps:\n                if hasattr(model_cls, \"target_var\") and model_cls.target_var:\n                    deps.append(model_cls.target_var)\n\n        # Recursively collect dependencies\n        for dep in deps:\n            collect(dep)\n\n        visiting.remove(var)\n        all_vars.add(var)\n\n    # Collect from all target variables\n    for var in target_variables:\n        collect(var)\n\n    # Topological sort using Kahn's algorithm\n    return _topological_sort(all_vars)\n</code></pre>"},{"location":"api_reference/#varframe.resolve_dependencies--only-specify-final-variables-dependencies-auto-resolved","title":"Only specify final variables - dependencies auto-resolved!","text":"<p>all_vars = resolve_dependencies([PredictedGapDelta])</p>"},{"location":"api_reference/#varframe.resolve_dependencies--returns-lap-gap-tireage-gapdelta-predictedgapdelta","title":"Returns: [Lap, Gap, TireAge, GapDelta, PredictedGapDelta]","text":""},{"location":"api_reference/#configuration","title":"Configuration","text":""},{"location":"api_reference/#varframe.VFConfig","title":"<code>varframe.VFConfig</code>","text":"<p>Global configuration for VarFrame warnings and implicit operations.</p> <p>Controls whether implicit operations (like auto-training models or computing variables not in _variables) issue warnings or are blocked entirely.</p> <p>This is a static configuration class - all attributes are class-level.</p> Class Attributes <p>warn_add_variable_no_compute (bool): Warn when adding variable without computing. warn_add_variable_compute (bool): Warn when computing variable not in _variables. warn_train_model (bool): Warn when auto-training a model. warn_infer_model (bool): Warn when inferring with a model implicitly. allow_implicit_train (bool): Allow implicit model training. allow_implicit_infer (bool): Allow implicit model inference. allow_implicit_compute (bool): Allow implicit variable computation. warnings_enabled (bool): Master switch for all warnings.</p> Example Source code in <code>varframe/config.py</code> <pre><code>class VFConfig:\n    \"\"\"\n    Global configuration for VarFrame warnings and implicit operations.\n\n    Controls whether implicit operations (like auto-training models or computing\n    variables not in _variables) issue warnings or are blocked entirely.\n\n    This is a static configuration class - all attributes are class-level.\n\n    Class Attributes:\n        warn_add_variable_no_compute (bool): Warn when adding variable without computing.\n        warn_add_variable_compute (bool): Warn when computing variable not in _variables.\n        warn_train_model (bool): Warn when auto-training a model.\n        warn_infer_model (bool): Warn when inferring with a model implicitly.\n        allow_implicit_train (bool): Allow implicit model training.\n        allow_implicit_infer (bool): Allow implicit model inference.\n        allow_implicit_compute (bool): Allow implicit variable computation.\n        warnings_enabled (bool): Master switch for all warnings.\n\n    Example:\n        &gt;&gt;&gt; # Disable all warnings\n        &gt;&gt;&gt; VFConfig.warnings_enabled = False\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Block implicit model training (raises RuntimeError)\n        &gt;&gt;&gt; VFConfig.allow_implicit_train = False\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Suppress specific warning type\n        &gt;&gt;&gt; VFConfig.warn_train_model = False\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Context manager for temporary suppression\n        &gt;&gt;&gt; with VFConfig.suppress_warnings():\n        ...     vf.resolve(PredictedGapDelta)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Reset to defaults\n        &gt;&gt;&gt; VFConfig.reset()\n    \"\"\"\n\n    # Warning flags - control which operations emit warnings\n    warn_add_variable_no_compute: ClassVar[bool] = True\n    warn_add_variable_compute: ClassVar[bool] = True\n    warn_train_model: ClassVar[bool] = True\n    warn_infer_model: ClassVar[bool] = True\n\n    # Permission flags - if False, raises error instead of warning\n    allow_implicit_train: ClassVar[bool] = True\n    allow_implicit_infer: ClassVar[bool] = True\n    allow_implicit_compute: ClassVar[bool] = True\n\n    # Master switches\n    warnings_enabled: ClassVar[bool] = True\n\n    # Internal state for context manager\n    _suppressed: ClassVar[bool] = False\n\n    @classmethod\n    def warn(\n        cls,\n        operation: ImplicitOperation,\n        message: str,\n        stacklevel: int = 3,\n    ) -&gt; None:\n        \"\"\"\n        Issue a warning for an implicit operation if configured.\n\n        Args:\n            operation: The type of implicit operation.\n            message: The warning message.\n            stacklevel: Stack level for the warning (default 3 for typical call depth).\n        \"\"\"\n        if cls._suppressed or not cls.warnings_enabled:\n            return\n\n        should_warn = {\n            ImplicitOperation.ADD_VARIABLE_NO_COMPUTE: cls.warn_add_variable_no_compute,\n            ImplicitOperation.ADD_VARIABLE_COMPUTE: cls.warn_add_variable_compute,\n            ImplicitOperation.TRAIN_MODEL: cls.warn_train_model,\n            ImplicitOperation.INFER_MODEL: cls.warn_infer_model,\n        }.get(operation, True)\n\n        if should_warn:\n            # Use orange/yellow color for visibility in terminal\n            colored_msg = f\"\\033[93m\u26a0 {message}\\033[0m\"\n            warnings.warn(colored_msg, UserWarning, stacklevel=stacklevel)\n\n    @classmethod\n    def check_permission(cls, operation: ImplicitOperation, context: str = \"\") -&gt; None:\n        \"\"\"\n        Check if an implicit operation is allowed.\n\n        Args:\n            operation: The type of implicit operation.\n            context: Additional context for the error message.\n\n        Raises:\n            RuntimeError: If the operation is not allowed.\n        \"\"\"\n        permission_map = {\n            ImplicitOperation.TRAIN_MODEL: (\n                cls.allow_implicit_train,\n                \"Implicit model training is disabled\",\n            ),\n            ImplicitOperation.INFER_MODEL: (\n                cls.allow_implicit_infer,\n                \"Implicit model inference is disabled\",\n            ),\n            ImplicitOperation.ADD_VARIABLE_COMPUTE: (\n                cls.allow_implicit_compute,\n                \"Implicit variable computation is disabled\",\n            ),\n            ImplicitOperation.ADD_VARIABLE_NO_COMPUTE: (\n                cls.allow_implicit_compute,\n                \"Implicit variable addition is disabled\",\n            ),\n        }\n\n        allowed, base_msg = permission_map.get(operation, (True, \"\"))\n        if not allowed:\n            full_msg = f\"{base_msg}. {context}\" if context else base_msg\n            raise RuntimeError(full_msg)\n\n    @classmethod\n    def suppress_warnings(cls) -&gt; \"_WarningSuppressionContext\":\n        \"\"\"\n        Context manager for temporarily suppressing all VF warnings.\n\n        Returns:\n            A context manager that suppresses warnings while active.\n\n        Example:\n            &gt;&gt;&gt; with VFConfig.suppress_warnings():\n            ...     vf.resolve(PredictedGapDelta)  # No warnings issued\n        \"\"\"\n        return _WarningSuppressionContext()\n\n    @classmethod\n    def reset(cls) -&gt; None:\n        \"\"\"Reset all configuration to default values.\"\"\"\n        cls.warn_add_variable_no_compute = True\n        cls.warn_add_variable_compute = True\n        cls.warn_train_model = True\n        cls.warn_infer_model = True\n        cls.allow_implicit_train = True\n        cls.allow_implicit_infer = True\n        cls.allow_implicit_compute = True\n        cls.warnings_enabled = True\n        cls._suppressed = False\n\n    @classmethod\n    def null_context(cls) -&gt; \"_NullContext\":\n        \"\"\"\n        Returns a context manager that does nothing.\n        \"\"\"\n        return _NullContext()\n</code></pre>"},{"location":"api_reference/#varframe.VFConfig--disable-all-warnings","title":"Disable all warnings","text":"<p>VFConfig.warnings_enabled = False</p>"},{"location":"api_reference/#varframe.VFConfig--block-implicit-model-training-raises-runtimeerror","title":"Block implicit model training (raises RuntimeError)","text":"<p>VFConfig.allow_implicit_train = False</p>"},{"location":"api_reference/#varframe.VFConfig--suppress-specific-warning-type","title":"Suppress specific warning type","text":"<p>VFConfig.warn_train_model = False</p>"},{"location":"api_reference/#varframe.VFConfig--context-manager-for-temporary-suppression","title":"Context manager for temporary suppression","text":"<p>with VFConfig.suppress_warnings(): ...     vf.resolve(PredictedGapDelta)</p>"},{"location":"api_reference/#varframe.VFConfig--reset-to-defaults","title":"Reset to defaults","text":"<p>VFConfig.reset()</p>"},{"location":"api_reference/#varframe.VFConfig.check_permission","title":"<code>check_permission(operation, context='')</code>  <code>classmethod</code>","text":"<p>Check if an implicit operation is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>ImplicitOperation</code> <p>The type of implicit operation.</p> required <code>context</code> <code>str</code> <p>Additional context for the error message.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the operation is not allowed.</p> Source code in <code>varframe/config.py</code> <pre><code>@classmethod\ndef check_permission(cls, operation: ImplicitOperation, context: str = \"\") -&gt; None:\n    \"\"\"\n    Check if an implicit operation is allowed.\n\n    Args:\n        operation: The type of implicit operation.\n        context: Additional context for the error message.\n\n    Raises:\n        RuntimeError: If the operation is not allowed.\n    \"\"\"\n    permission_map = {\n        ImplicitOperation.TRAIN_MODEL: (\n            cls.allow_implicit_train,\n            \"Implicit model training is disabled\",\n        ),\n        ImplicitOperation.INFER_MODEL: (\n            cls.allow_implicit_infer,\n            \"Implicit model inference is disabled\",\n        ),\n        ImplicitOperation.ADD_VARIABLE_COMPUTE: (\n            cls.allow_implicit_compute,\n            \"Implicit variable computation is disabled\",\n        ),\n        ImplicitOperation.ADD_VARIABLE_NO_COMPUTE: (\n            cls.allow_implicit_compute,\n            \"Implicit variable addition is disabled\",\n        ),\n    }\n\n    allowed, base_msg = permission_map.get(operation, (True, \"\"))\n    if not allowed:\n        full_msg = f\"{base_msg}. {context}\" if context else base_msg\n        raise RuntimeError(full_msg)\n</code></pre>"},{"location":"api_reference/#varframe.VFConfig.null_context","title":"<code>null_context()</code>  <code>classmethod</code>","text":"<p>Returns a context manager that does nothing.</p> Source code in <code>varframe/config.py</code> <pre><code>@classmethod\ndef null_context(cls) -&gt; \"_NullContext\":\n    \"\"\"\n    Returns a context manager that does nothing.\n    \"\"\"\n    return _NullContext()\n</code></pre>"},{"location":"api_reference/#varframe.VFConfig.reset","title":"<code>reset()</code>  <code>classmethod</code>","text":"<p>Reset all configuration to default values.</p> Source code in <code>varframe/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset all configuration to default values.\"\"\"\n    cls.warn_add_variable_no_compute = True\n    cls.warn_add_variable_compute = True\n    cls.warn_train_model = True\n    cls.warn_infer_model = True\n    cls.allow_implicit_train = True\n    cls.allow_implicit_infer = True\n    cls.allow_implicit_compute = True\n    cls.warnings_enabled = True\n    cls._suppressed = False\n</code></pre>"},{"location":"api_reference/#varframe.VFConfig.suppress_warnings","title":"<code>suppress_warnings()</code>  <code>classmethod</code>","text":"<p>Context manager for temporarily suppressing all VF warnings.</p> <p>Returns:</p> Type Description <code>'_WarningSuppressionContext'</code> <p>A context manager that suppresses warnings while active.</p> Example <p>with VFConfig.suppress_warnings(): ...     vf.resolve(PredictedGapDelta)  # No warnings issued</p> Source code in <code>varframe/config.py</code> <pre><code>@classmethod\ndef suppress_warnings(cls) -&gt; \"_WarningSuppressionContext\":\n    \"\"\"\n    Context manager for temporarily suppressing all VF warnings.\n\n    Returns:\n        A context manager that suppresses warnings while active.\n\n    Example:\n        &gt;&gt;&gt; with VFConfig.suppress_warnings():\n        ...     vf.resolve(PredictedGapDelta)  # No warnings issued\n    \"\"\"\n    return _WarningSuppressionContext()\n</code></pre>"},{"location":"api_reference/#varframe.VFConfig.warn","title":"<code>warn(operation, message, stacklevel=3)</code>  <code>classmethod</code>","text":"<p>Issue a warning for an implicit operation if configured.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>ImplicitOperation</code> <p>The type of implicit operation.</p> required <code>message</code> <code>str</code> <p>The warning message.</p> required <code>stacklevel</code> <code>int</code> <p>Stack level for the warning (default 3 for typical call depth).</p> <code>3</code> Source code in <code>varframe/config.py</code> <pre><code>@classmethod\ndef warn(\n    cls,\n    operation: ImplicitOperation,\n    message: str,\n    stacklevel: int = 3,\n) -&gt; None:\n    \"\"\"\n    Issue a warning for an implicit operation if configured.\n\n    Args:\n        operation: The type of implicit operation.\n        message: The warning message.\n        stacklevel: Stack level for the warning (default 3 for typical call depth).\n    \"\"\"\n    if cls._suppressed or not cls.warnings_enabled:\n        return\n\n    should_warn = {\n        ImplicitOperation.ADD_VARIABLE_NO_COMPUTE: cls.warn_add_variable_no_compute,\n        ImplicitOperation.ADD_VARIABLE_COMPUTE: cls.warn_add_variable_compute,\n        ImplicitOperation.TRAIN_MODEL: cls.warn_train_model,\n        ImplicitOperation.INFER_MODEL: cls.warn_infer_model,\n    }.get(operation, True)\n\n    if should_warn:\n        # Use orange/yellow color for visibility in terminal\n        colored_msg = f\"\\033[93m\u26a0 {message}\\033[0m\"\n        warnings.warn(colored_msg, UserWarning, stacklevel=stacklevel)\n</code></pre>"},{"location":"api_reference/#varframe.ImplicitOperation","title":"<code>varframe.ImplicitOperation</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for different implicit operations that can be warned or blocked.</p> <p>These represent operations that happen automatically in the library, which users may want to be notified about or prevent entirely.</p> <p>Attributes:</p> Name Type Description <code>ADD_VARIABLE_NO_COMPUTE</code> <p>Adding a variable to registry without computing it.</p> <code>ADD_VARIABLE_COMPUTE</code> <p>Computing a variable not explicitly in the registry.</p> <code>TRAIN_MODEL</code> <p>Automatically training a model that hasn't been trained.</p> <code>INFER_MODEL</code> <p>Performing inference with a model.</p> Source code in <code>varframe/config.py</code> <pre><code>class ImplicitOperation(Enum):\n    \"\"\"\n    Enum for different implicit operations that can be warned or blocked.\n\n    These represent operations that happen automatically in the library,\n    which users may want to be notified about or prevent entirely.\n\n    Attributes:\n        ADD_VARIABLE_NO_COMPUTE: Adding a variable to registry without computing it.\n        ADD_VARIABLE_COMPUTE: Computing a variable not explicitly in the registry.\n        TRAIN_MODEL: Automatically training a model that hasn't been trained.\n        INFER_MODEL: Performing inference with a model.\n    \"\"\"\n\n    ADD_VARIABLE_NO_COMPUTE = auto()\n    ADD_VARIABLE_COMPUTE = auto()\n    TRAIN_MODEL = auto()\n    INFER_MODEL = auto()\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":"<p>This guide details the core components and workflows of VarFrame. We will build a complete pipeline to predict Customer Lifetime Value (LTV).</p> <p>You can follow along step-by-step. A complete, runnable script is provided at the bottom of this page.</p>"},{"location":"user_guide/#1-setup-imports","title":"1. Setup &amp; Imports","text":"<p>First, we import the necessary components. VarFrame integrates seamlessly with pandas and scikit-learn.</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom varframe import VarFrame, BaseVariable, DerivedVariable, BaseModel, ModelVariable\n</code></pre>"},{"location":"user_guide/#2-base-variables","title":"2. Base Variables","text":"<p>Base Variables represent the columns in your raw input data. They act as the \"contract\" for your pipeline, ensuring the input data has the expected structure.</p> <ul> <li><code>raw_column</code>: The exact name of the column in your input DataFrame.</li> <li><code>dtype</code>: (Optional) The type to cast the data to.</li> </ul> <pre><code>class AvgOrderValue(BaseVariable):\n    raw_column = \"avg_order_value\"\n    dtype = \"float\"\n\nclass PurchaseFrequency(BaseVariable):\n    raw_column = \"purchase_frequency\"\n    dtype = \"float\"\n</code></pre>"},{"location":"user_guide/#3-derived-variables","title":"3. Derived Variables","text":"<p>Derived Variables are calculated from other variables. This is where you define your feature engineering logic.</p> <ul> <li><code>dependencies</code>: A list of other variable classes this variable needs. VarFrame uses this to determine execution order.</li> <li><code>calculate(cls, df)</code>: The method that performs the transformation.</li> </ul> <pre><code>class AnnualRevenue(DerivedVariable):\n    dependencies = [AvgOrderValue, PurchaseFrequency]\n\n    @classmethod\n    def calculate(cls, df):\n        # We access data using .name to be safe and consistent\n        return df[AvgOrderValue.name] * df[PurchaseFrequency.name]\n</code></pre>"},{"location":"user_guide/#4-lazy-loading","title":"4. Lazy Loading","text":"<p>Sometimes a variable is expensive to compute (e.g., API calls, complex simulations) and not always needed. You can mark these as Lazy.</p> <ul> <li><code>lazy = True</code>: The variable is not computed by default when you run <code>resolve()</code>.</li> <li>It is computed only when you explicitly ask for it (e.g., <code>vf[CustomerScore]</code>).</li> </ul> <pre><code>class CustomerScore(DerivedVariable):\n    dependencies = [AnnualRevenue]\n    lazy = True\n\n    @classmethod\n    def calculate(cls, df):\n        print(\"Performing expensive calculation for Customer Score...\")\n        return df[AnnualRevenue.name] * 0.1\n</code></pre>"},{"location":"user_guide/#5-machine-learning-models","title":"5. Machine Learning Models","text":"<p>VarFrame treats ML models as first-class citizens in the dependency graph.</p>"},{"location":"user_guide/#defining-the-model","title":"Defining the Model","text":"<p><code>BaseModel</code> defines the schema of your model: what goes in, what comes out, and what algorithm to use.</p> <pre><code># The target variable we want to predict (for training)\nclass LifetimeValue(BaseVariable):\n    name = \"lifetime_value\"\n    raw_column = \"lifetime_value\"\n\nclass LTVPredictor(BaseModel):\n    name = \"ltv_predictor\"\n    input_vars = [AvgOrderValue, PurchaseFrequency, AnnualRevenue]\n    target_var = LifetimeValue\n    model_class = LinearRegression\n</code></pre>"},{"location":"user_guide/#getting-predictions","title":"Getting Predictions","text":"<p><code>ModelVariable</code> represents the output of the model.</p> <pre><code>class PredictedLTV(ModelVariable):\n    name = \"predicted_ltv\"\n    model_class = LTVPredictor\n</code></pre> <p>Auto-Training: If <code>PredictedLTV</code> is requested but <code>LTVPredictor</code> hasn't been trained, VarFrame will automatically train the model using the current DataFrame (if <code>target_var</code> is present).</p>"},{"location":"user_guide/#6-execution","title":"6. Execution","text":"<p>Now we initialize the <code>VarFrame</code> with some raw data and resolve our target variables.</p> <pre><code># 1. Initialize with raw data\ndata = pd.DataFrame({\n    \"avg_order_value\": [50.0, 100.0, 20.0, 80.0],\n    \"purchase_frequency\": [4, 2, 10, 1],\n    \"lifetime_value\": [220.0, 210.0, 250.0, 90.0]\n})\n\nvf = VarFrame(data)\n\n# 2. Resolve Dependencies\n# We asks for PredictedLTV. VarFrame figures out the rest:\n# AnnualRevenue -&gt; Train LTVPredictor -&gt; Predict LTV\nvf.resolve(PredictedLTV)\n\nprint(vf[[AvgOrderValue, AnnualRevenue, PredictedLTV]])\n</code></pre>"},{"location":"user_guide/#7-using-lazy-variables","title":"7. Using Lazy Variables","text":"<p>Lazy variables remain uncomputed until accessed.</p> <pre><code>print(f\"Is score computed? {'customerscore' in vf.columns}\")\n\n# Accessing triggers the calculation\nscore = vf[CustomerScore]\n\nprint(f\"Is score computed now? {'customerscore' in vf.columns}\")\n</code></pre>"},{"location":"user_guide/#8-import-export","title":"8. Import / Export","text":"<p>When exporting, you can force all lazy variables to be computed using <code>include=['all']</code>.</p> <pre><code># Export everything, including lazy variables\nvf.to_csv(\"customer_ltv.csv\", index=False, include=['all'])\n</code></pre>"},{"location":"user_guide/#9-data-integrity-hashing","title":"9. Data Integrity &amp; Hashing","text":"<p>VarFrame ensures that the data you load matches the code you are running. When exporting to Parquet, it embeds a recursive hash of the variable definitions (logic, dependencies, attributes).</p> <p>When loading, it compares these hashes with your current code and warns you of discrepancies:</p> <ul> <li>Logic Change (Red): The calculation method or source column has changed.</li> <li>Dependency Change (Red): A variable this depends on has changed.</li> <li>Type Change (Orange): The data type (<code>dtype</code>) has changed.</li> <li>Metadata Change (White): Description or Lazy flag changed (usually safe).</li> </ul> <pre><code># Warns if 'AnnualRevenue' logic changed since export\nvf = VarFrame.load_parquet(\"pipeline_v1.parquet\")\n</code></pre>"},{"location":"user_guide/#10-advanced-loading-options","title":"10. Advanced Loading Options","text":"<p>Both <code>load_csv</code> and <code>load_parquet</code> support powerful filtering and disambiguation:</p>"},{"location":"user_guide/#column-selection","title":"Column Selection","text":"<pre><code># Whitelist: Only load specific variables\nvf = VarFrame.load_csv(\"data.csv\", variables=[AvgOrderValue, AnnualRevenue])\n\n# Blacklist: Exclude certain variables\nvf = VarFrame.load_parquet(\"data.parquet\", exclude=[LegacyVariable])\n</code></pre>"},{"location":"user_guide/#handling-ambiguity","title":"Handling Ambiguity","text":"<p>When multiple variable classes share the same <code>name</code>, you must disambiguate:</p> <pre><code># Explicit disambiguation\nvf = VarFrame.load_csv(\"data.csv\", ambiguity={\"revenue\": AnnualRevenueV2})\n</code></pre> <p>If ambiguity is not resolved, an <code>AmbiguityError</code> is raised.</p>"},{"location":"user_guide/#unmatched-columns","title":"Unmatched Columns","text":"<p>By default, columns not matching any known variable are dropped. To keep them:</p> <pre><code>vf = VarFrame.load_csv(\"data.csv\", discard_unmatched=False)\n</code></pre>"},{"location":"user_guide/#full-runnable-code","title":"Full Runnable Code","text":"<p>Here is the complete script combining all the steps above.</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom varframe import VarFrame, BaseVariable, DerivedVariable, BaseModel, ModelVariable\n\n# --- Variables ---\nclass AvgOrderValue(BaseVariable):\n    raw_column = \"avg_order_value\"\n\nclass PurchaseFrequency(BaseVariable):\n    raw_column = \"purchase_frequency\"\n\nclass AnnualRevenue(DerivedVariable):\n    dependencies = [AvgOrderValue, PurchaseFrequency]\n    @classmethod\n    def calculate(cls, df):\n        return df[AvgOrderValue.name] * df[PurchaseFrequency.name]\n\nclass CustomerScore(DerivedVariable):\n    dependencies = [AnnualRevenue]\n    lazy = True\n    @classmethod\n    def calculate(cls, df):\n        return df[AnnualRevenue.name] * 0.1\n\n# --- Model ---\nclass LifetimeValue(BaseVariable):\n    name = \"lifetime_value\"\n    raw_column = \"lifetime_value\"\n\nclass LTVPredictor(BaseModel):\n    name = \"ltv_predictor\"\n    input_vars = [AvgOrderValue, PurchaseFrequency, AnnualRevenue]\n    target_var = LifetimeValue\n    model_class = LinearRegression\n\nclass PredictedLTV(ModelVariable):\n    name = \"predicted_ltv\"\n    model_class = LTVPredictor\n\n# --- Execution ---\nif __name__ == \"__main__\":\n    data = pd.DataFrame({\n        \"avg_order_value\": [50.0, 100.0, 20.0, 80.0],\n        \"purchase_frequency\": [4, 2, 10, 1],\n        \"lifetime_value\": [220.0, 210.0, 250.0, 90.0]\n    })\n\n    vf = VarFrame(data)\n\n    # 1. Resolve &amp; Auto-Train\n    vf.resolve(PredictedLTV)\n    print(\"Predictions:\\n\", vf[PredictedLTV])\n\n    # 2. Lazy Load\n    print(\"\\nCustomer Score (JIT):\\n\", vf[CustomerScore])\n\n    # 3. Export\n    vf.to_csv(\"output.csv\", include=['all'])\n    print(\"\\nExported pipeline results.\")\n</code></pre>"}]}